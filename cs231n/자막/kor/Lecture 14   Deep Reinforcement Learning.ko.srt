1
00:00:09,784 --> 00:00:11,867
시작하겠습니다!

2
00:00:13,038 --> 00:00:15,888
CS231n 14강에 오신 것을 환영합니다.

3
00:00:15,888 --> 00:00:20,884
이번 시간에는 강화학습을 배워보겠습니다.

4
00:00:20,884 --> 00:00:23,222
우선 공지사항을 전달하겠습니다.

5
00:00:23,222 --> 00:00:30,346
어제 저녁 중간고사 점수를 공개했습니다.
자세한 정보는 Piazza에서 확인하시기 바랍니다.

6
00:00:30,346 --> 00:00:35,402
그리고 두 번째 과제 및 마일스톤 점수는
주말 중으로 공개하도록 하겠습니다.

7
00:00:36,768 --> 00:00:40,682
프로젝트와 관련한 전달사항도 있습니다.
모든 팀은 반드시 프로젝트를 등록하셔야 합니다.

8
00:00:40,682 --> 00:00:47,580
Piazza에서 양식 확인 후 작성 바라며

9
00:00:47,580 --> 00:00:53,214
여러분이 작성하신 문서는 최종 학점에 반영되며
포스터 세션에서 활용 할 예정입니다.

10
00:00:53,214 --> 00:01:01,779
Tiny ImageNet Challenge에 쓰일 서버가 열렸습니다.

11
00:01:01,779 --> 00:01:06,193
그리고 며칠전 Piazza에 CS231n 수업평가
설문을 게시하였습니다.

12
00:01:06,193 --> 00:01:13,600
아직 진행하지 않으신 분들께서는 설문을 부탁드리며
CS231n의 성장을 위한 아주 소중한 피드백이 될 것입니다.

13
00:01:16,589 --> 00:01:19,650
자 오늘의 주제는 강화학습(reinforcement learning) 입니다.

14
00:01:19,650 --> 00:01:22,544
우리는 지금까지는
supervised learning을 배웠습니다.

15
00:01:22,544 --> 00:01:30,498
데이터인 x와 레이블인 y가 있었고, x를 y에 매핑하는
함수를 학습하는게 목표였습니다.

16
00:01:30,498 --> 00:01:35,067
가령 classification 문제가 있었습니다.

17
00:01:35,067 --> 00:01:37,753
그리고 지난 강의에서는
unsupervised learning을 배웠습니다.

18
00:01:37,753 --> 00:01:45,362
데이터는 있지만 레이블이 없는 경우였습니다. 데이터에 내재된
숨겨진 구조를 학습하는게 목표였습니다.

19
00:01:45,362 --> 00:01:50,528
가령 생성모델(generative models)이 있었습니다.

20
00:01:52,040 --> 00:01:57,370
이번에 배우는 것은 조금 다릅니다.
강화학습이라는 문제입니다.

21
00:01:57,370 --> 00:02:01,824
여기에는 에이전트(agent)가 있습니다. 환경(environment)
에서 행동(action)을 취하는 주체입니다.

22
00:02:01,824 --> 00:02:04,352
에이전트는 행동에 따른
적절한 보상(rewards)을 받습니다.

23
00:02:04,352 --> 00:02:09,959
강화학습은 에이전트의 보상을 최대화할 수 있는
행동이 무엇인지를 학습하는 것입니다.

24
00:02:09,959 --> 00:02:14,101
오늘은 이에 대해서 자세하게 다뤄볼 것입니다.

25
00:02:14,101 --> 00:02:18,116
이번 수업의 개요를 말씀드리자면
우선 강화학습이 어떤 문제인지를 다루고

26
00:02:18,116 --> 00:02:20,927
그리고 Markov Decision Processes(MDP)
에 대해서 다룰 것입니다.

27
00:02:20,927 --> 00:02:24,747
MDP는 강화학습 문제의 수식체계(formalism) 입니다.

28
00:02:24,747 --> 00:02:31,095
그리고 대표적인 강화학습 방법인 Q-Learning과
Policy Gradients에 대해서 배워보겠습니다.

29
00:02:32,876 --> 00:02:38,936
강화학습의 문제에서는 에이전트와 환경이 있습니다.

30
00:02:38,936 --> 00:02:43,268
환경에서 에이전트에게는 상태가 주어집니다.

31
00:02:43,268 --> 00:02:46,877
그리고 에이전트는 어떤 행동을 취하게 됩니다.

32
00:02:46,877 --> 00:02:52,609
그러면 환경은 행동에 따라 에이전트에게
보상을 주고 다음 상태를 부여합니다.

33
00:02:52,609 --> 00:03:00,918
이 과정은 에이전트가 종료상태(terminal state)가
될 때 까지 반복됩니다. 종료가 되면 한 에피소드가 끝나는 것이죠

34
00:03:00,918 --> 00:03:03,401
자 그럼 몇 가지 예시를 한번 살펴보겠습니다.

35
00:03:03,401 --> 00:03:05,536
우선 "cart-pole" 문제입니다.

36
00:03:05,536 --> 00:03:11,142
"cart-pole"는 아주 고전적인 문제입니다.
이미 CS229에서 접해보신 분들도 계실 것입니다.

37
00:03:11,142 --> 00:03:16,252
"cart-pole" 문제의 목표는 움직이는 카트(cart)와 카트 위에
매달려있는 막대기(pole)의 균형을 유지하는 것입니다.

38
00:03:16,252 --> 00:03:20,280
상태(state)에는 현재 시스템이 기술되어있습니다.

39
00:03:20,280 --> 00:03:28,206
가령, 막대기의 각,막대기의 각속도,
카트의 위치,카트의 수평속도가 있습니다.

40
00:03:28,206 --> 00:03:33,224
에이전트는 카트를 수평으로 미는 행동을 취할 수 있습니다.
(horizontal forces)

41
00:03:33,224 --> 00:03:38,387
우리는 카트를 밀면서 동시에
막대기의 균형을 잘 유지해야 합니다.

42
00:03:38,387 --> 00:03:43,990
환경으로 부터 받을 수 있는 보상은
"막대기가 제대로 서 있으면 1점" 입니다.

43
00:03:43,990 --> 00:03:48,143
여러분은 막대기를 가능한 똑바로 세워야 합니다.

44
00:03:49,286 --> 00:03:52,192
고전적인 RL문제에서 다뤘던 다양한 예시들을 살펴보겠습니다.

45
00:03:52,192 --> 00:03:53,998
"로봇 보행(robot locomotion)"  문제가 있습니다.

46
00:03:53,998 --> 00:03:59,670
여기 휴머노이드 로봇 모델과
개미 로봇 모델이 있습니다.

47
00:03:59,670 --> 00:04:03,128
로봇이 앞으로 나아가도록 하는 것이 목표입니다.

48
00:04:03,128 --> 00:04:10,807
이 문제에서 상태(state)는 로봇의
모든 관절들의 각과 위치입니다.

49
00:04:10,807 --> 00:04:15,887
에이전트가 취할 수 있는 행동(Action)은
각 관절들에 가해지는 토크(torques) 입니다.

50
00:04:15,887 --> 00:04:21,228
이 문제에서 하고싶은 것은 로봇이 앞으로 전진하는 것입니다.
앞으로 이동하면 보상을 받고

51
00:04:21,228 --> 00:04:31,701
휴머노이드 로봇의 경우에는 로봇에 똑바로 서 있는
경우에도 추가적인 보상을 받습니다.

52
00:04:33,521 --> 00:04:38,384
RL로 게임 문제도 풀 수 있습니다.

53
00:04:38,384 --> 00:04:40,700
가령 여기 아타리 게임(Atrari games)이 있습니다.

54
00:04:40,700 --> 00:04:44,280
깊은 강화학습(Deep reinforcement learning)가
아주 큰 성과를 이룬 종목이기도 합니다.

55
00:04:44,280 --> 00:04:48,574
아타리 게임에서는 가능한 가장 높은 점수로
게임을 끝마치는 것이 목적입니다.

56
00:04:48,574 --> 00:04:52,753
에이전트가 게임 플레이어가 되어
게임을 진행하게 됩니다.

57
00:04:52,753 --> 00:04:57,506
현재 게임 진행 화면 그대로
(raw pixels)가 상태로 주어집니다.

58
00:04:57,506 --> 00:05:02,882
우리가 게임을 할 때 보이는 화면
그 자체가 상태인 것이죠

59
00:05:02,882 --> 00:05:09,912
에이전트의 행동은, 우리가 게임 플레이할 때 처럼
위, 아래, 좌, 우로 움직일 수 있습니다.

60
00:05:09,912 --> 00:05:15,667
매 스텝마다 게임 점수를 획를할 수도, 잃을 수도 있죠

61
00:05:15,667 --> 00:05:27,572
게임 종료 시점까지 점수를 최대화 하는 것이 목표입니다.
그리고 마지막으로 살펴볼 게임은 바로 "바둑" 입니다.

62
00:05:27,573 --> 00:05:31,697
지난해 강화학습이 이룬 가장 큰 성과라고 할 수 있습니다.

63
00:05:31,697 --> 00:05:38,588
세계 최고의 바둑기사였던 이세돌을 딥마인드의
알파고가 꺽은 엄청난 사건이 있었습니다.

64
00:05:38,589 --> 00:05:50,918
그리고 알파고는 최근에 또 다른 바둑
최강자들을 상대로 대국을 준비하고 있습니다.

65
00:05:50,919 --> 00:05:56,295
바둑에서는 게임에서 이기는 것이 목표입니다.
바둑을 둘 수 있는 모든 자리가 상태가 됩니다.

66
00:05:56,295 --> 00:06:03,911
행동은 다음 수를 두는 것이며
게임을 이겼을 경우에만 보상이 주어집니다.

67
00:06:03,912 --> 00:06:08,411
이에 대해서는 잠시 후에
자세히 살펴보도록 하겠습니다.

68
00:06:08,411 --> 00:06:13,329
그렇다면 강화학습 문제를 어떤 식으로
수학적으로 나타내 볼 수 있을까요?

69
00:06:13,330 --> 00:06:18,051
앞서 보았던 것 처럼 환경은
에이전트에게 상태를 부여합니다.

70
00:06:18,051 --> 00:06:20,634
그러면 에이전트는 행동을 취합니다.

71
00:06:22,394 --> 00:06:28,512
Markov Decision Process를 통해서
강화학습 문제를 수식화 시킬 수 있습니다.

72
00:06:28,512 --> 00:06:31,447
MDP는 Markov property를 만족합니다.

73
00:06:31,447 --> 00:06:36,107
Markov property란 현재 상태만으로
전체 상태를 나타내는 성질입니다.

74
00:06:36,107 --> 00:06:40,164
그리고 MDP는 여기 보시는 것과 같이
몇 가지 속성으로 정의가 되는데

75
00:06:40,164 --> 00:06:43,170
S는 가능한 상태들의 집합입니다.

76
00:06:43,170 --> 00:06:45,762
A는 가능한 행동들의 집합입니다.

77
00:06:45,762 --> 00:06:51,694
R은 (state, action) 쌍이 주어졌을 때 받게되는
보상의 분포가 되겠습니다.

78
00:06:51,694 --> 00:06:55,323
따라서 R은 (state, action)이 보상으로 매핑되는 함수입니다.

79
00:06:55,323 --> 00:06:57,430
P는 전이확률(transition probability) 입니다.

80
00:06:57,430 --> 00:07:02,940
(state, action) 쌍이 주어졌을때 전이 될
다음 상대에 대한 분포를 나타냅니다.

81
00:07:02,940 --> 00:07:05,718
마지막으로 감마는  discount factor입니다.

82
00:07:05,718 --> 00:07:12,970
discount factor는 보상을 받는 시간에 대해서
우리가 얼마나 중요하게 생각할 것인지 말해줍니다.

83
00:07:14,203 --> 00:07:17,395
MDP가 작동하는 방식을 한번 살펴보겠습니다.

84
00:07:17,395 --> 00:07:20,053
우선 초기 time step인 t = 0입니다.

85
00:07:20,053 --> 00:07:26,362
환경은 초기 초기 상태 분포인
p(s_0)에서 상태 s_0 를 샘플링합니다.

86
00:07:26,363 --> 00:07:32,253
그리고 t=0 에서부터 완료 상태가 될 때 까지
아래를 반복합니다.

87
00:07:32,253 --> 00:07:35,797
에이전트가 행동 a_t를 선택합니다.

88
00:07:35,797 --> 00:07:38,885
환경은 어떤 분포로부터 보상을 샘플링합니다.

89
00:07:38,885 --> 00:07:44,032
보상은 우리의 상태와 우리가 택한 행동이
주어졌을때의 보상입니다.

90
00:07:44,032 --> 00:07:51,534
환경은 다음 어떤 분포에서 상태인 s_t+1도 샘플링합니다.

91
00:07:51,534 --> 00:07:58,706
그리고 에이전트는 보상과 다음 상태를 받습니다.
그리고 다음 단계를 수행하게 됩니다.

92
00:07:58,707 --> 00:08:05,542
에이전트는 에피소트가 종료될 때 까지 이런 식으로
보상과 다음 상태를 받는 과정을 되풀이합니다.

93
00:08:05,542 --> 00:08:06,989
좋습니다.

94
00:08:06,989 --> 00:08:10,724
자 이제는 정책(polity) pi를 정의할 수 있습니다.

95
00:08:10,724 --> 00:08:16,651
정책은 각 상태에서 에이전트가 어떤 행동을 취할지를
명시해주는 기능을 수행합니다.

96
00:08:16,651 --> 00:08:19,748
정책은 deterministic 할수도
stochastic 할수도 있습니다.

97
00:08:19,748 --> 00:08:27,204
이제 우리의 목적은 최적의 정책 pi^star를 찾는 것입니다. 즉,
cumulative discounted reward를 최대화시키는 것입니다.

98
00:08:27,205 --> 00:08:35,508
보상에는 미래에 얻을 보상도 포함이 되는데 이 보상은
discount factor에 의해 할인된 보상을 얻습니다.

99
00:08:35,509 --> 00:08:39,327
자 그럼 아주 간단한 MDP 예제를 살펴보겠습니다.

100
00:08:39,327 --> 00:08:44,533
여기 예시로 격자로 된 Grid World가 있습니다.
이 곳에서 테스크를 수행해 봅시다.

101
00:08:44,533 --> 00:08:50,295
여기 격자 중 어디로든 이동할 수 있습니다.
우리의 상태가 될 것입니다.

102
00:08:50,295 --> 00:08:52,613
우리는 상태에 따라서 행동을 취할 수 있습니다.

103
00:08:52,613 --> 00:08:59,298
햄동은 간단한 움직임이 될 것입니다.
상,하,좌,우로 움직이는 행동을 취할 수 있겠죠

104
00:08:59,299 --> 00:09:08,858
여러분은 한번 움직일 때 마다 음의 보상
(negative reward)를 받게 됩니다.

105
00:09:08,859 --> 00:09:11,989
가령 R = -1 이 될 수도 있겠죠

106
00:09:11,989 --> 00:09:20,054
여러분의 목표는 여기 회색으로 칠해진 "종료
상태" 에 최소한의 행동으로 도달하는 것입니다.

107
00:09:20,055 --> 00:09:27,624
종료 상태에 도달하는 시간이 길어질수록
음의 보상이 점점 쌓이게 될 것입니다.

108
00:09:27,625 --> 00:09:30,540
그럼 random policy부터 살펴보겠습니다.

109
00:09:30,540 --> 00:09:39,089
random policy 에서는 기본적으로 어떤
방향로 움직이든 무작위로 방향을 결정합니다.

110
00:09:39,090 --> 00:09:41,843
모든 방향이 동일한 확률을 갖습니다.

111
00:09:41,843 --> 00:09:46,518
하지만 우리가 일련의 학습을 거쳐서 얻게될
optimal policy의 경우에는

112
00:09:46,518 --> 00:09:51,866
우리가 점점 더 종료 상태에 가까워 지도록 만드는
적절한 방향을 선택해서 행동을 취하게 됩니다.

113
00:09:51,866 --> 00:09:59,170
가령 종료 상태 바로 주변에 위치하는 경우라면
모든 방향은 종료 상태로 바로 이동하게끔 합니다.

114
00:09:59,171 --> 00:10:09,118
종료 상태와 먼 곳에 있다고 하더라도, 종료 상태에
가장 가깝게 이동할 수 있는 방향으로 이동합니다.

115
00:10:09,119 --> 00:10:17,154
이렇게 MDP를 정의하고 나면 최적의
정책인 pi^star를 찾아야 합니다.

116
00:10:17,155 --> 00:10:20,755
최적의 정책은 보상의 합을 최대화시킵니다.

117
00:10:20,755 --> 00:10:29,730
최적의 정책은 우리가 어떤 상태에 있더라도 그 상황에서
보상을 최대화시킬 수 있는 행동을 알려줍니다.

118
00:10:29,731 --> 00:10:34,091
자 그럼 질문은 "MDP에서 발생하는 무작위성(randomness)
을 어떻게 다뤄야 할까요?"

119
00:10:34,091 --> 00:10:39,073
가령 초기 상태를 샘플링할 시에도 무작위성이 있고

120
00:10:39,073 --> 00:10:46,340
전이 확률 분포의 경우에도 다음 상태가 확률적입니다.

121
00:10:46,341 --> 00:10:51,947
이를 위해서는 보상의 합에 대한 기댓값을 최대화시키면 됩니다.

122
00:10:51,947 --> 00:11:02,956
수식적으로 써보면, 최적의 정책 pi^star는 정책 pi에
대한 미래의 보상들의 합의 기댓값을 최대화시키는 것입니다.

123
00:11:02,957 --> 00:11:05,103
여기에서 초기 상태(s_0)는 어떤 상태 분포를 따르며

124
00:11:05,103 --> 00:11:09,127
우리가 취하는 행동은 어떤 상태가 주어졌을 때
정책이 가지는 분포로부터 샘플링됩니다.

125
00:11:09,127 --> 00:11:16,423
마지막으로 다음 상태는 전이 확률
분포로부터 샘플링됩니다.

126
00:11:16,423 --> 00:11:22,142
자 그럼 최적의 정책을 찾는 방법을 다루기 이전에

127
00:11:22,143 --> 00:11:26,787
앞으로 사용하게 될 몇 가지 정의부터 살펴보겠습니다.

128
00:11:26,787 --> 00:11:31,405
가치 함수(value function)와
Q-가치 함수(Q-value function)입니다.

129
00:11:31,405 --> 00:11:37,425
우리가 정책을 따라 무언가를 수행하게 되면 결국은
모든 에피소드마다 어떤 "경로" 를 얻게 될 것입니다.

130
00:11:37,426 --> 00:11:43,611
초기 상태인 s_0, a_0, r_0 부터 시작해서
s_1, a_1, r_1 이런식으로 쭉쭉 나가겠죠

131
00:11:43,611 --> 00:11:49,331
그렇게 되면 우리가 얻을 수 있는 상태(s), 행동(a), 보상(r)
들의 하나의 경로(trajectory)가 생깁니다.

132
00:11:49,331 --> 00:11:52,613
그렇다면 우리가 현재 속해있는
상태가 얼마나 좋은 상태일까요?

133
00:11:52,613 --> 00:12:10,799
임의의 상태 s에 대한 가치함수는 상태 s와 정책
pi가 주어졌을 때 누적 보상의 기댓값 입니다.

134
00:12:10,800 --> 00:12:13,286
그렇다면 (상태, 행동) 쌍이 얼마나 좋은지는
어떻게 알 수 있을까요?

135
00:12:13,286 --> 00:12:17,370
상태 s에서 어떤 행동 a를 취하는게 좋은 것일까요?

136
00:12:17,370 --> 00:12:20,468
우리는 이를 Q-가치 함수를 통해서 정의할 수 있습니다.

137
00:12:20,468 --> 00:12:27,741
Q-가치 함수는 정책 pi, 행동 a, 상태 s가 주어졌을 때
받을 수 있는 누적 보상의 기댓값입니다.

138
00:12:29,708 --> 00:12:45,098
최적의 Q-가치 함수인 Q^star는 (상태, 행동)
쌍으로부터 얻을 수 있는 누적 보상의 기댓값을 최대화시킵니다.

139
00:12:45,099 --> 00:12:52,017
다음으로는 강화학습에서 중요한 요소 중 하나인 벨만
방정식(Bellman equation)에 대해서 알아보겠습니다.

140
00:12:52,018 --> 00:12:57,697
우선 최적의 정책으로부터 나온 Q-가치 함수인
Q^star가 있다고 가정해봅시다.

141
00:12:57,697 --> 00:13:00,911
Q^star는 벨만 방정식을 만족할 것입니다.

142
00:13:00,911 --> 00:13:05,194
이것이 의미하는 바는

143
00:13:05,194 --> 00:13:11,748
어떤 (s, a)이 주어지던 간에

144
00:13:11,748 --> 00:13:18,867
현재 (s, a)에서 받을 수 있는 r 과 에피소드가
종료될 s^prime까지의 보상을 더한 값입니다.

145
00:13:18,868 --> 00:13:24,150
여기에서는 우리가 이미 최적의 정책을 알고 있기 때문에

146
00:13:24,150 --> 00:13:28,746
따라서 s^prime에서 우리가 할 수 있는
최상의 행동을 취할 수가 있습니다.

147
00:13:28,746 --> 00:13:34,432
s^prime에서의 Q^star의 값은
우리가 현재 상태에서 취할 수 있는 모든 행동들 중에

148
00:13:34,432 --> 00:13:38,626
Q^star(s^prime, a^prime) 을 최대화시키는 값이 됩니다.

149
00:13:38,626 --> 00:13:44,119
이를 통해서 최적의 Q 값을 얻을 수 있습니다.

150
00:13:44,119 --> 00:13:54,251
그리고 우리가 어떤 상태인지에 대한 무작위성이
존재하기 때문에 여기에도 기댓값을 취합니다.

151
00:13:54,252 --> 00:14:02,487
우리는 또한 Q^star를 통해서 특정 상태에서의 최상의
행동을 취할 수 있는 최적의 정책을 구할 수 있습니다.

152
00:14:02,488 --> 00:14:08,436
Q^star는 어떤 행동을 취했을때
미래에 받을 보상의 최대치입니다.

153
00:14:08,437 --> 00:14:16,862
따라서 우리는 그저 정책을 따라 행동을
취하기만 하면 최상의 보상을 받을 수 있습니다.

154
00:14:16,863 --> 00:14:21,025
그렇다면 어떻게 최적의 정책을 구할 수 있을까요?

155
00:14:21,025 --> 00:14:25,692
우리가 이 문제를 해결할 수 있는 한 가지 방법은 바로
value iteration algorithm 입니다.

156
00:14:25,692 --> 00:14:29,527
반복적인 업데이트를 위해서 벨만 방정식을 사용할 것입니다.

157
00:14:29,527 --> 00:14:37,997
Bellman equation을 통해 각 스텝마다 Q_star를
조금씩 최적화시키면 됩니다.

158
00:14:39,347 --> 00:14:50,830
수학적으로 보면 Q_i의 i가 무한대일때
최적의 Q_star로 수렴하게 됩니다.

159
00:14:54,257 --> 00:14:58,329
이 방법이 잘 동작하긴 할 겁니다.
하지만 문제가 있습니다. 무엇일까요?

160
00:14:59,184 --> 00:15:02,720
문제는 바로 이 방법은
scalable 않다는 점입니다.

161
00:15:02,720 --> 00:15:08,596
반복적으로 업데이트하기 위해서는 모든 (상태, 행동)
마다 Q(s)를 계산해야만 합니다.

162
00:15:08,597 --> 00:15:18,932
가령 Atati 게임의 경우 스크린에
보이는 모든 픽셀이 상태가 될 수 있습니다.

163
00:15:18,933 --> 00:15:28,724
이 경우 상태 공간이 아주 크며, 기본적으로
전체 상태 공간을 계산하는 것은 불가능합니다.

164
00:15:28,725 --> 00:15:35,907
해결책은 무엇일까요? 우선, 함수 Q(s, a)
를 근사시킬 수 있습니다.

165
00:15:35,908 --> 00:15:37,620
예를 들어 neural network를 사용할 수 있겠죠

166
00:15:37,620 --> 00:15:48,471
언제나 그러 했듯이, 우리가 모르는 아주 복잡한 함수를
추정하고 싶을때는 neural network가 제격입니다.

167
00:15:48,472 --> 00:15:54,242
그러면 이제 Q-learning 수식을 살펴보겠습니다.

168
00:15:54,242 --> 00:16:02,950
여기에서는 행동 가치 함수(action value
function)을 추정하기 위한 함수 근사를 이용할 것입니다.

169
00:16:02,951 --> 00:16:08,141
함수 근사로는 최근 가장 많이 사용되는
deep neural network를 이용할 것입니다.

170
00:16:08,142 --> 00:16:10,782
이를 deep Q-learning 이라고 합니다.

171
00:16:10,782 --> 00:16:20,149
Deep Q-learning은 요즘 강화학습
하면 빠지지 않고 등장하는 방법입니다.

172
00:16:20,150 --> 00:16:30,765
여기에서는 함수 파라미터 theta가 있습니다.
theta는 neural network의 가중치입니다.

173
00:16:33,050 --> 00:16:37,970
자 그럼 함수 근사를 이용해서
어떻 방식으로 최적의 정책을 찾아낼 수 있을까요?

174
00:16:37,970 --> 00:16:44,744
Q-function은 벨만 방정식을 만족해야 한다는 점을 명심해야 합니다.

175
00:16:44,744 --> 00:16:54,712
자 이제 우리 함수가 벨만 방정식을 만족하도록 해야 합니다. 해야할
일은 neural network로 근사시킨 Q-function을

176
00:16:54,713 --> 00:17:00,239
학습시켜서 벨만 방정식의 에러가
최소가 되도록 하면 됩니다.

177
00:17:00,240 --> 00:17:03,689
손실 함수는 q(s, a)가 목적 함수와 얼마나 멀리
떨어져있는지 측정합니다.

178
00:17:03,689 --> 00:17:09,853
여기 보이는 목적함수 y_i가 바로 앞서 살펴본
벨만 방적식이 되겠습니다.

179
00:17:09,853 --> 00:17:16,928
forward pass에서는 손실 함수를 계산합니다.
손실이 최소가 되면 좋겠죠

180
00:17:16,929 --> 00:17:28,182
backward pass에서는 계산한 손실을
기반으로 파라미터 theta를 업데이트합니다.

181
00:17:28,183 --> 00:17:38,435
이런 식의 반복적인 업데이트를 총해서 우리가 가진
Q-function이 타켓과 가까워지도록 학습시킵니다.

182
00:17:38,436 --> 00:17:43,524
질문 있나요?

183
00:17:44,537 --> 00:17:53,369
Deep Q-learning 방법이
적용된 고전적인 예제를 살펴보겠습니다.

184
00:17:53,370 --> 00:18:01,745
앞서 살펴본 Atrari game 입니다. 게임의
목적은 최대한 높은 점수를 획득하는 것입니다.

185
00:18:01,746 --> 00:18:05,460
상태는 게임의 픽셀이 그대로 사용되었습니다.

186
00:18:05,460 --> 00:18:12,963
행동은 상, 하, 좌, 우 와 같이
게임에 필요한 어떤 행동이 될 것입니다.

187
00:18:12,964 --> 00:18:21,182
게임이 진행됨에 따라 매 타임 스템마다 점수가
늘어나거나 줄어듦에 따라 보상을 얻습니다.

188
00:18:21,183 --> 00:18:27,095
스코어를 기반으로 전체 누적 보상을 계산할 수 있습니다.

189
00:18:27,095 --> 00:18:32,955
Q-function에 사용한 네트워크는 다음과 같이 생겼습니다.

190
00:18:32,955 --> 00:18:37,355
Q-network는 가중치인 theta를 가지고 있습니다.

191
00:18:37,355 --> 00:18:43,791
네트워크의 입력은 상태 s 입니다.
즉 현제 게임 스크린의 픽셀들이 들어옵니다.

192
00:18:43,791 --> 00:18:49,509
실제로는 4프레임 정도 누적시켜서 사용합니다.

193
00:18:49,509 --> 00:18:58,608
입력은 RGB->grayscale변환, 다운
샘플링, 크라핑 등 전처리 과정을 거칩니다.

194
00:18:58,609 --> 00:19:04,631
전처리를 거친 입력은 최근 4 프레임을 누적시킨
84 x 84 x 4 의 형태가 됩니다.

195
00:19:04,631 --> 00:19:05,464
질문 있나요?

196
00:19:05,464 --> 00:19:09,631
[학생이 질문]

197
00:19:12,792 --> 00:19:20,808
질문은 "네트워크가 다양한 (상태, 행동) 쌍들을 구하기
위해서 Q-가치 함수를 근사시키는 것인지" 입니다.

198
00:19:20,809 --> 00:19:24,765
(가령 이 예시의 경우 4개의 (상태, 행동)쌍을 출력)
네 맞습니다.

199
00:19:24,765 --> 00:19:27,551
이에 대해서는 다음 슬라이드에서
조금 더 자세히 설명할 것입니다.

200
00:19:27,551 --> 00:19:29,935
[학생이 질문]

201
00:19:29,935 --> 00:19:36,815
여기에서는 Softmax를 사용하지는 않습니다. Q-value
functions을 직접 예측하는 것이 목적입니다.

202
00:19:36,816 --> 00:19:40,582
[학생이 질문]

203
00:19:40,583 --> 00:19:44,014
Q-values를 regression 한다고 보시면 됩니다.

204
00:19:44,014 --> 00:19:54,083
입력 다음으로 살펴볼 것은 네트워크 구조인데, 우리에게는 아주
익숙한 구조입니다. 컨볼루션, FC 레이어들로 구성됩니다.

205
00:19:54,084 --> 00:19:59,610
8 x 8 / 4 x 4 컨볼루션이 있고

206
00:19:59,611 --> 00:20:05,673
그리고 FC 256 레이어를 거칩니다. 이제는
우리에게 아주 익숙한 네트워크 구조입니다.

207
00:20:05,674 --> 00:20:17,414
FC 레이어의 출력 벡터는 네트워크의 입력인, "상태"
가 주어졌을 때 각 행동의 Q-value 입니다.

208
00:20:17,415 --> 00:20:21,770
가령 네 가지 행동이 존재한다면
출력도 4차원입니다.

209
00:20:21,770 --> 00:20:28,685
이는 현재 상태 s_t와 여기에 존재하는 행동들
a_1, a_2, a_3, a_4에 관한 Q 값들입니다.

210
00:20:28,685 --> 00:20:33,179
각 행동들 마다 하나의 스칼라 값을 얻습니다.

211
00:20:33,179 --> 00:20:43,072
참고로 행동의 수는 Atari game의 종류에
따라서 4~18가지로 변할 수도 있습니다.

212
00:20:43,073 --> 00:20:46,709
이런 방식의 네트워크 구조의 장점은

213
00:20:46,709 --> 00:20:54,650
한 번의 forward pass 만으로 현재 상태에 해당하는
모든 함수에 대한 Q-value를 계산할 수 있다는 점입니다.

214
00:20:54,651 --> 00:20:56,117
이는 정말 효율적인 방법입니다.

215
00:20:56,117 --> 00:21:05,945
현재 상태의 각 행동들에 각각 Q-value가 존재할텐데
현재 상태를 네트워크의 입력으로 넣어주기만 하면

216
00:21:05,946 --> 00:21:10,259
모든 Q-value를 한번의 forward pass로 계산할 수 있습니다.

217
00:21:10,259 --> 00:21:15,078
이 네트워크를 학습시키 위해서는
앞서 살펴본 손실 함수가 필요합니다.

218
00:21:15,078 --> 00:21:17,661
네트워크로 근사시킨 함수도
벨만 방정식을 만족해야 합니다.

219
00:21:17,661 --> 00:21:29,314
따라서 네트워크의 출력인 Q-value가 타겟
값과 가까워 지도록 반복적으로 학습시켜야 합니다.

220
00:21:29,315 --> 00:21:40,705
backward pass에서는 손실함수를 기반으로
그레디언트를 계산하고 이를 바탕으로 네트워크를 학습시킵니다.

221
00:21:40,706 --> 00:21:45,639
한 가지 알아야 할 개념이 있습니다.
"experience replay" 라는 것입니다.

222
00:21:45,639 --> 00:21:53,440
Experience Replay는 Q-network에서
발생할 수 있는 문제들을 다룹니다.

223
00:21:53,440 --> 00:21:58,134
Q-network를 학습시킬 때 하나의 배치에서
시간적으로 연속적인 샘플들로 학습하면 안좋습니다.

224
00:21:58,134 --> 00:22:09,409
가령 게임을 진행하면서 시간적으로 연속적인 샘플들을
이용해서 상태, 행동, 보상을 계산하고 학습시키면

225
00:22:09,410 --> 00:22:16,117
모든 샘플들이 상관 관계를 가집니다(correlated). 이는
학습에 아주 비효율적입니다. 이 문제가 첫 번째 문제입니다.

226
00:22:16,118 --> 00:22:24,841
그리고 두번째로는, 현재 Q-network 파라미터를 생각해보면
네트워크는 우리가 어떤 행동을 해야할지에 정책을 결정한다는 것은

227
00:22:24,842 --> 00:22:27,394
우리가 다음 샘플들도 결정하게 된다는 의미입니다.

228
00:22:27,394 --> 00:22:30,832
이는 학습에 좋지 않은 영향을 미칠 것입니다.
(bad feedback loops)

229
00:22:30,832 --> 00:22:35,468
가령 현재 상태에서 "왼쪽" 으로 이동하는 것이
보상을 최대화하는 행동이라면

230
00:22:35,468 --> 00:22:43,305
결국 다음 샘플들도 전부 "왼쪽"에서
발생할 수 있는 것들로만 편향될 것입니다.

231
00:22:43,306 --> 00:22:45,406
이는 문제가 될 수 있습니다.

232
00:22:45,406 --> 00:22:53,097
이를 해결하는 방법으로 "experience
replay" 라는 방법을 사용합니다.

233
00:22:53,098 --> 00:23:01,352
이 방법은 replay memory를 이용합니다. replay memory에는
(상태, 행동, 보상. 다음상태)로 구성된 전이 테이블이 있습니다.

234
00:23:01,353 --> 00:23:08,772
게임 에피소드를 플레이하면서 더 많은 경험을 얻음에
따라 전이 데이블을 지속적으로 업데이트시킵니다.

235
00:23:08,773 --> 00:23:16,334
여기에서는 replay memory에서의 임의의 미니
배치를 이용하여 Q-network를 학습시킵니다.

236
00:23:16,335 --> 00:23:24,826
연속적인 샘플을 사용하는 대신 전이 테이블에서
임의로 샘플링된 샘플을 사용하는 것이죠.

237
00:23:24,827 --> 00:23:31,007
이 방식을 통해서 앞서 상관관계(correlation)로
발생하는 문제를 해결할 수 있을 것입니다.

238
00:23:31,007 --> 00:23:39,206
추가적인 이점으로는 각각의 전이가 가중치 업데이트에 여러
차례(딱 한번이 아니라) 기여할 수 있다는 점입니다.

239
00:23:39,207 --> 00:23:43,652
전이 테이블로부터 샘플링을 하면
하나의 샘플도 여러번  뽑힐 수 있는 것이죠

240
00:23:43,652 --> 00:23:47,585
이를 통해 데이터 효율이 훨씬 더 증가하게 됩니다.

241
00:23:50,580 --> 00:23:59,165
자 그럼 종합해서, experience replay를
이용한 Q-learning 전체 알고리즘을 살펴보겠습니다.

242
00:23:59,166 --> 00:24:03,940
우선  replay memory을 초기화하는 것부터 시작합니다.

243
00:24:03,940 --> 00:24:14,829
우선 memory capacity인 N을 정해줍니다.
그리고 Q-network를 임의의 가중치로 초기화시킵니다.

244
00:24:14,830 --> 00:24:21,832
그리고 앞으로 M번의 에피소드를 진행할 것입니다.
따라서 학습은 총 M번의 에피소드까지 진행됩니다.

245
00:24:21,832 --> 00:24:31,264
그리고 각 에피소드마다 상태를 초기화시켜야 합니다.
상태는 게임 시작화면 픽셀이 될 것입니다.

246
00:24:31,265 --> 00:24:37,814
앞서, 입력 상태를 만들기 위한
전처리 과정은 앞서 언급했었죠

247
00:24:37,814 --> 00:24:41,584
다음은, 게임이 진행중인
매 타음 스탭마다

248
00:24:41,584 --> 00:24:46,268
적은 확률로는 임의의 행동을 취할 것입니다.
(정책을 따르지 않고)

249
00:24:46,268 --> 00:24:53,141
이는 충분한 탐사(exploration)을 위해서는
아주 중요한 부분입니다.

250
00:24:53,141 --> 00:24:58,559
이를 통해 다양한 상태 공간을 샘플링할 수 있습니다.

251
00:24:58,559 --> 00:25:03,613
낮은 확률로 임의의 행동을 취하거나, 또는(otherwise)
현재 정책에 따라 greedy action을 취합니다.

252
00:25:03,614 --> 00:25:13,579
다시 말해 대부분의 경우에는 현재 상태에 적합하다고 판단되는
greedy action을 선택하게 될 것이고, 가끔식은

253
00:25:13,580 --> 00:25:16,300
낮은 확률로 임의의 행동이 선택될 것입니다.

254
00:25:16,300 --> 00:25:26,069
이렇게 행동 a_t를 취하면 보상(r_t)과
다음 상태(s_t+1)를 얻을 수 있습니다.

255
00:25:26,070 --> 00:25:32,771
그러고 나서 전이(transition)을
replay memory에 저장합니다.

256
00:25:32,771 --> 00:25:35,577
자 이제 네트워크를 학습시킬 차례입니다.

257
00:25:35,577 --> 00:25:37,550
experience replay를 이용할 시점입니다.

258
00:25:37,550 --> 00:25:47,213
replay memory에서 임의의 미니배치 전이들(transitions)을
샘플링한 다음에 이를 이용하여 업데이트합니다.

259
00:25:47,214 --> 00:25:49,635
이것이 전체 학습의 반복과정입니다.

260
00:25:49,635 --> 00:26:03,886
게임을 진행하는 동안에 experienced replay을 이용하여
미니배치를 샘플링하고, 이를 통해 Q-network를 학습시킵니다.

261
00:26:03,887 --> 00:26:20,910
Google DeepMind에서 Atari 게임 Breakout을
Q-learning으로 학습시키는 영상을 살펴보겠습니다.

262
00:26:20,911 --> 00:26:26,185
게임화면의 픽셀 그대로가 상태입니다.

263
00:26:26,185 --> 00:26:29,520
학습 초반에 어떤 일이 발생하는지 살펴봅시다.

264
00:26:29,520 --> 00:26:40,302
학습 초반에는, 공을 치긴 하지만 그렇게 잘해보이지는 않습니다.
금방 놓치고 맙니다.

265
00:26:40,303 --> 00:26:42,886
그래도 계속 공을 찾아다니고 있습니다.

266
00:26:50,969 --> 00:26:55,737
그리고 몇 시간정도 더 학습한 결과를 살펴봅시다.

267
00:27:00,946 --> 00:27:05,113
지금은 어느정도 잘 하는 것 같습니다.

268
00:27:06,190 --> 00:27:16,592
공도 잘 따라가고 있고 블럭도 거의 다 깨는 모습입니다.

269
00:27:16,593 --> 00:27:36,203
240분이 지나면 이제는 거의
프로에 가까운 묘기를 선보입니다.

270
00:27:36,203 --> 00:27:42,795
공을 위로 올려서 알아서 블럭이 깨지게 하는 전략이죠

271
00:27:42,796 --> 00:27:49,500
지금 보여드린 예시에서는 Agent가 deep Q-learning을
통해 Atari 게임을 학습하고 플레이하였습니다.

272
00:27:49,501 --> 00:27:56,418
온라인을 통해서 더 많은 Atrari
게임에서의 예시를 확인해보실 수 있습니다.

273
00:27:56,419 --> 00:28:01,149
지금까지 Q-learning을 알아봤습니다.
하지만 Q-learning에는 문제가 있었습니다.

274
00:28:01,149 --> 00:28:07,225
아주 어려운 문제였는데요, 그 문제는 바로
Q-function이 너무나도 복잡하다는 것입니다.

275
00:28:07,226 --> 00:28:12,335
Q-learning 에서는 모든 (state, action) 쌍들을
학습해야만 합니다.

276
00:28:12,335 --> 00:28:17,275
그런데 가령, 로봇이 어떤 문체를 손에 쥐는
문제를 풀어야 한다고 생각해봅시다.

277
00:28:17,275 --> 00:28:19,576
이 경우 아주 고차원의 상태 공간이 존재합니다.

278
00:28:19,576 --> 00:28:26,225
가령 로봇의 모든 관절의 위치와 각도가 이룰 수 있는
모든 경우의 수를 생각해볼 수 있겠죠

279
00:28:26,225 --> 00:28:35,492
이 모든 (state, action)을
학습시키는것은 아주 어려운 문제입니다.

280
00:28:35,493 --> 00:28:38,724
다른 한편으로는 "정책" 자체는 훨씬 더 간단할 수 있습니다.

281
00:28:38,724 --> 00:28:44,555
정책은 간단하게 "손을 움켜쥐는 것" 같이 말이죠

282
00:28:44,556 --> 00:28:48,252
손가락을 특정 방향으로 움직이기만 하면 됩니다.

283
00:28:48,252 --> 00:28:54,142
그렇다면 정책 자체를 학습시킬 수 있을까요?

284
00:28:54,142 --> 00:28:58,306
만약 가능하다면 여러 정책들 가운데
최고의 정책을 찾아낼 수 있을 것입니다.

285
00:28:58,306 --> 00:29:06,789
정책을 결정하기에 앞서 Q-value를
추정하는 과정을 거치지 않고도 말이죠

286
00:29:06,790 --> 00:29:15,937
이러한 접근 방식이
"policy gradients" 입니다.

287
00:29:15,938 --> 00:29:20,858
자 그럼 수식적으로 매개변수화된 정책 집합
(class of parametrized policies)을 정의해봅시다.

288
00:29:20,858 --> 00:29:24,146
정책들은 가중치 theta에 의해서 매개변수화됩니다.

289
00:29:24,146 --> 00:29:27,791
자 그럼 각 정책에 대해서
정책의 값을 정의해봅시다.

290
00:29:27,791 --> 00:29:35,722
J(theta)는 미래에 받을 보상들의
누적 합의 기댓값으로 나타낼 수 있습니다.

291
00:29:35,723 --> 00:29:38,971
이는 우리가 지금까지 사용했던
보상과 동일합니다.

292
00:29:38,971 --> 00:29:41,879
이런 상황(setup)에서 우리가 하고싶은 것은

293
00:29:41,879 --> 00:29:51,547
최적의 정책인 theta^star를 찾는 것인데 이는
argmax_theta J(theta)로 나타낼 수 있습니다.

294
00:29:51,548 --> 00:29:56,917
이제는 보상의 기댓값을 최대로 하는 정책
파라미터를 찾으면 됩니다.

295
00:29:56,917 --> 00:30:01,011
이 문제를 어떻게 풀면 될까요?

296
00:30:04,993 --> 00:30:10,155
policy parameter에 대해서
gradient ascent를 수행하면 될 것입니다.

297
00:30:10,155 --> 00:30:15,460
지금까지 배웠던 것 처럼 하면 되겠죠
gradient ascent를 통해서

298
00:30:15,460 --> 00:30:20,762
parameters를 연속적으로 업데이트해주면 됩니다.

299
00:30:23,202 --> 00:30:29,196
자 그럼 조금만 더 구체적으로 살펴보겠습니다.
REINFORCE 알고리즘을 한번 알아봅시다.

300
00:30:29,196 --> 00:30:36,781
수학적으로 보면, 경로에 대한 미래 보상의
기댓값으로 나타낼 수 있습니다.

301
00:30:36,781 --> 00:30:41,902
이를 위해서 경로를 샘플링해야 하는데, 가령 이는 앞선 예제에서
게임 플레이 시의 에피소드일 수 있습니다.

302
00:30:41,902 --> 00:30:47,411
s_0, a_0, r_0, s_1 등등 이 되겠죠

303
00:30:47,411 --> 00:30:51,723
이들은 어떤 정책 pi_theta을 따라 결정될 것입니다.

304
00:30:51,723 --> 00:30:57,739
그러면 각 경로에 대해서 보상을 계산할 수 있을 것입니다.

305
00:30:57,739 --> 00:31:01,245
이 보상은 우리가 어떤 경로를 따라 얻을
수 있는 누적 보상이 될 것입니다.

306
00:31:01,245 --> 00:31:10,570
따라서 정책인 pi_theta의 값은 샘플링 된 경로로 부터
받게 될 보상의 기댓값이 될 것입니다.

307
00:31:10,570 --> 00:31:16,868
따라서 여기에 이 기댓값은 우리가 정책으로 부터
샘플링한 경로들에 대한 기댓값입니다.

308
00:31:18,563 --> 00:31:21,288
자 그럼 gradient ascent를 수행해야 합니다.

309
00:31:21,288 --> 00:31:22,961
J(theta)를 미분해봅시다.

310
00:31:22,961 --> 00:31:27,356
미분을 하고나서 이 값들을
gradient step으로 취하면 될 것입니다.

311
00:31:28,535 --> 00:31:34,300
하지만 여기에도 문제가 있습니다. 미분을 잘 하긴 했는데
이 값을 계산할 수가 없습니다(intractable).

312
00:31:34,300 --> 00:31:41,319
p가 theta에 종속되어 있는 상황에서
기댓값 안에 그레디언트가 있으면 문제가 될 수 있습니다.

313
00:31:41,319 --> 00:31:47,661
여기에서 보면 p(tau ; thera)에 대한
그레디언트를 구해야하는데

314
00:31:47,661 --> 00:31:53,033
tau에 대한 적분을 수행해야 하기 때문에
계산하기기 어렵습니다.

315
00:31:53,033 --> 00:31:57,327
하지만 이를 해결하기 위한 트릭이 있습니다.

316
00:31:57,327 --> 00:32:04,941
이 트릭은 p를 가지고 하는 것인데
어짜피 1을 곱하면 상관없으므로

317
00:32:04,941 --> 00:32:10,286
위 아래에 p(tau ; theta)를 곱해줍니다.

318
00:32:10,286 --> 00:32:26,170
자 이제 아래 쪽 두 번째, 세 번째 수식을 보시면
이 둘은 동치(equvalent)입니다.

319
00:32:26,170 --> 00:32:32,741
왜냐하면 log(p)의 그레디언트는
(1 / p) * p의 그레디언트 이기 때문입니다.

320
00:32:33,808 --> 00:32:41,385
자 그럼 이 수식을 앞에 있던
그레디언트 식에 대입해 봅시다.

321
00:32:41,385 --> 00:32:43,426
자 이제 실제로 어떻게 보이는지를 살펴봅시다.

322
00:32:43,426 --> 00:32:52,187
이제는 log(p)에 대한 그레디언트를 모든 경로에 대한
확률과 곱하는 꼴이 되고, 이를 tau에 대해서 적분하는 꼴이 되기 때문에

323
00:32:52,187 --> 00:32:58,586
이는 다시 경로 tau에 대한 기댓값의 형식으로 바꿀 수 있습니다.

324
00:32:58,586 --> 00:33:02,751
처음에는 기댓값에 대한 그레디언트를 계산했지만

325
00:33:02,751 --> 00:33:06,823
이제는 그레디언트에 대한 기댓값으로 바꾼 셈입니다.

326
00:33:06,823 --> 00:33:13,817
이로 인해, 그레디언트를 추정하기 위해서
경로들 샘플링하여 사용할 수 있게 되었습니다.

327
00:33:14,712 --> 00:33:21,260
따라서 이제는 Monte Carlo 샘플링을 사용할 수 있습니다.
이것이 바로 REINFORCE의 주요 아이디어입니다.

328
00:33:23,624 --> 00:33:28,180
자 그럼 우리가 계산하려고 하는 식을 살펴보겠습니다.

329
00:33:28,180 --> 00:33:33,071
그렇다면 우리는 p(tau ; theta)를
전이확률을 모르는 채로 계산할 수 있을까요?

330
00:33:33,071 --> 00:33:38,466
자 우선  p(tau)는 어떤 경로에 대한 확률이 될 것입니다.

331
00:33:38,466 --> 00:33:45,821
이는 현재 (상태, 행동)이 주어졌을 때, 다음에 얻게될
모든 상태에 대해서 "전이확률"과

332
00:33:45,821 --> 00:33:52,232
"정책 pi로 부터 얻은 행동에 대한 확률"의 곱의 형태로 이루어집니다.

333
00:33:52,232 --> 00:33:58,441
따라서 이를 모두 곱하면 경로에 대한
확률을 얻어낼 수 있을 것입니다.

334
00:33:58,441 --> 00:34:10,389
그렇다면 log(p ; tau)의 경우를 보면 앞서 곱했던 것들이
모두 합의 형태로 바뀌게 될 것입 니다.

335
00:34:10,389 --> 00:34:18,161
자 그럼 log(p)를 theta에 대해서 미분하는 경우를 생각해 봅시다.

336
00:34:18,163 --> 00:34:22,850
여기 보면 첫 번째 항인 전이확률은
theta와 무관한 항입니다.

337
00:34:22,850 --> 00:34:28,709
theta와 관련이 있는 항은 오직 두번째 항인
log pi_theta(a_t | s_t) 항 뿐입니다.

338
00:34:29,675 --> 00:34:39,669
우리는 이를 통해서 전이확률은 전혀 상관하지 않아도
된다는 사실을 알 수 있습니다.

339
00:34:39,670 --> 00:34:46,421
다시 말해, 그레디언트를 계산할 때
전이확률은 필요하지 않습니다.

340
00:34:47,257 --> 00:34:57,264
따라서 우리는 어떤 경로  tau에 대해서도
그레디언트를 기반으로 J(thera)을 추정해 낼 수 있을 것입니다.

341
00:34:58,524 --> 00:35:02,220
지금까지는 하나의 경로만 고려했었지만

342
00:35:02,220 --> 00:35:07,188
기댓값을 계산하기 위해서는 여러 개의 경로를
샘플링할 수도 있을 것입니다.

343
00:35:09,248 --> 00:35:17,141
그레디언트 계산을 하기 위해서 지금까지
유도해본 것들을 한번 해석해 봅시다.

344
00:35:18,217 --> 00:35:25,226
어떤 경로로부터 얻은 보상이 크다면, 즉
어떤 일련의 행동들이 잘 수행한 것이라면

345
00:35:25,226 --> 00:35:29,434
그 일련들 행동들을 할 확률을 높혀줍니다.

346
00:35:29,434 --> 00:35:33,141
그 행동들이 좋았다는 것을 말해주는 것입니다.

347
00:35:33,141 --> 00:35:37,186
반면 어떤 경로에 대한 보상이 낮다면
해당 확률을 낮춥니다.

348
00:35:37,186 --> 00:35:40,747
그러한 행동들은 좋지 못한 행동이었으며 따라서
그 경로가 샘플링되지 못하도록 해야 합니다.

349
00:35:40,747 --> 00:35:50,980
여기 수식을 보시면 pi(a given s)는
우리가 취한 행동들에 대한 우도(likelihood) 입니다.

350
00:35:50,980 --> 00:36:03,575
파라미터를 조정하기 위해서 그레디언트를 사용할 것이고,
그레디언트는 우도를 높히려면 어떻게 해야 하는지를 알려줍니다.

351
00:36:03,575 --> 00:36:12,602
즉 우리가 받게될 보상, 즉 행동들이 얼마나 좋았는지에 대한
그레디언트를 통해 파라미터를 조정하는 것입니다.

352
00:36:14,561 --> 00:36:23,798
단순하게 말해서, 어떤 경로가 좋았으면
경로에 포함되었던 모든 행동이 좋았다는 것을 의미합니다.

353
00:36:23,798 --> 00:36:26,356
하지만 기댓값이 의해서 이 모든 것들이
averages out 됩니다.

354
00:36:26,356 --> 00:36:30,125
average out을 통해
unbiased extimator를 얻을 수 있고

355
00:36:30,125 --> 00:36:35,622
충분히 많은 샘플링을 한다면 그레디언트를 잘 이용해서
정확하고 좋은 extimater를 얻을 수 있을 것입니다.

356
00:36:35,622 --> 00:36:42,994
이 방법이 좋은 이유는 그레디언트만 잘 계산한다면
손실 함수를 작게 만들 수 있고

357
00:36:42,994 --> 00:36:48,602
정책 파라미터 theta에 대한 (적어도) local optimum
을 구할 수 있기 때문엡니다.

358
00:36:48,602 --> 00:36:54,884
하지만 여기에도 문제가 존재합니다.
문제의 원인은 바로 바로 높은 분산에 있습니다.

359
00:36:54,884 --> 00:36:57,201
왜냐하면 신뢰할당문제(credit assignment)가
아주 어렵기 때문입니다.

360
00:36:57,201 --> 00:37:04,412
일단 보상을 받았으면  해당 경로의 모든
행동들이 좋았다 라는 정보만 알려줄 것입니다.

361
00:37:04,412 --> 00:37:11,080
하지만 우리는 구체적으로 어떤 행동이 최선이었는지를
알고 싶을 수도 있지만, 이 정보는 average out 됩니다.

362
00:37:11,080 --> 00:37:17,190
하지만 구체적으로 어떤 행동이 좋았는지 알 길이 없으며,
좋은 estimateor를 위해서는 샘플링을 충분히 하는 수 밖에 없습니다.

363
00:37:17,190 --> 00:37:23,851
이 문제는 결국 분산을 줄이고 estimator의 성능을 높이기 위해서는
어떻게 해야 하는지에 관한 질문으로 귀결됩니다.

364
00:37:26,540 --> 00:37:33,323
분산을 줄이는 것은 policy gradient 에서
아주 중요한 분야입니다.

365
00:37:33,323 --> 00:37:39,756
샘플링을 더 적게 하면서도 estimator의
성능을 높힐 수 있는 방법이기도 합니다.

366
00:37:39,756 --> 00:37:43,278
이를 해결할 수 있는 몇 가지 아이디어를 소개해 드리겠습니다.

367
00:37:44,202 --> 00:37:46,764
주어진 그래디언트 추정기를 보면,

368
00:37:46,764 --> 00:37:57,091
첫번째 아이디어는, 해당 상태로부터 받을 미래의 보상만을
고려하여 어떤 행동을 취할 확률을 키워주는 방법입니다.

369
00:37:57,091 --> 00:38:04,736
이 방법은 해당 경로에서 얻을 수 있는
전체 보상을 고려하는 것 대신에

370
00:38:04,736 --> 00:38:12,108
맨 처음부터가 아닌,  현재의 time step에서 부터 종료 시점까지
얻을 수 있는 보상의 합을 고려하는 것입니다.

371
00:38:12,108 --> 00:38:18,999
이 방법이 의도하는 바는 어떤 행동이 발생시키는
미래의 보상이 얼마나 큰지를 고려하겠다는 것입니다.

372
00:38:18,999 --> 00:38:20,499
그럴듯한 방법이죠

373
00:38:21,811 --> 00:38:29,448
두번째 아이디어는 지연된 보상에 대해서
할인률을 적용하는 것입니다.

374
00:38:29,448 --> 00:38:36,774
수식을 보시면 할인율이 추가되었습니다.
예전에도 봤었죠.

375
00:38:36,774 --> 00:38:47,276
할인율이 의미하는 것은. 당장 받을 수 있는 보상과
조금은 더 늦게 받은 보상간의 차이를 구별하는 것입니다.

376
00:38:47,276 --> 00:38:57,489
두번째 방법은 어떤 행동이 좋은 행동인지 아닌지를
해당 행동에 가까운 곳에서 찾습니다.

377
00:38:57,489 --> 00:39:00,880
나중에 수행하는 행동에 대해서는
가중치를 조금 낮추는 거죠

378
00:39:00,880 --> 00:39:07,730
이 두가지 방법은 실제로도 일반적으로 많이
사용하기도 하는 아주 쉬운 아이디어입니다.

379
00:39:07,730 --> 00:39:14,597
분산을 줄이기 위한 세번째 아이디어는
baseline이라는 방법입니다.

380
00:39:14,597 --> 00:39:20,690
경로에서부터 계산한 값을
그대로 사용하는 것은 문제가 있습니다.

381
00:39:21,675 --> 00:39:23,869
그런 값들 자체가 반드시 의미있는 값은 아닐 수도 있기 때문입니다.

382
00:39:23,869 --> 00:39:29,835
가량 보상이 모두 양수(positive)이기만 해도
행동들에 대한 확률이 계속 커지기만 할 것입니다.

383
00:39:29,835 --> 00:39:32,039
물론 그것이 나름대로 의미가 있을 순 있지만

384
00:39:32,039 --> 00:39:39,753
정말 중요한 것은 얻은 보상이 우리가 얻을 것이라고
예상했던 것 보다 더 좋은지, 아닌지를 판단하는 것입니다.

385
00:39:39,753 --> 00:39:46,071
이를 다루기 위해서 baseline function을 사용할 수 있습니다.
이 방법은 상태를 이용하는 방법입니다.

386
00:39:46,071 --> 00:39:53,886
baseline 함수가 말하고자 하는 것은 해당 상태에서
우리가 얼마 만큼의 보상을 원하는지 입니다(기준).

387
00:39:55,515 --> 00:39:59,837
그렇게 되면 확률을 키우거나 줄이는 보상 수식이 조금 바뀌게 되는데

388
00:39:59,837 --> 00:40:05,508
이를 수식에 적용하면 미래에 얻을 보상들의 합을
특정 기준이 되는 값(baseline)에서 값을 빼주는 형태가 되며

389
00:40:05,508 --> 00:40:10,772
이를 통해 우리가 기대했던 것에 비해 보상이 좋은지,
아닌지에 대한 상대적인 값을 얻을 수 있습니다.

390
00:40:11,870 --> 00:40:16,099
그렇다면 baseline을 어떻게 선택하면 좋을까요?

391
00:40:16,099 --> 00:40:19,168
가장 단순한 baseline은

392
00:40:19,168 --> 00:40:23,013
지금까지 경험했던 보상들에 대해서
moving average를 취하는 것입니다.

393
00:40:23,013 --> 00:40:34,765
에피소드를 수행하는 학습 과정에서 지금까지 봤던 모든 경로들에
대해서  보상이 어땠는지에 대한 평균을 내는 것입니다.

394
00:40:34,765 --> 00:40:41,716
이 아이디어를 이용해서 현재 보상이 상대적으로
좋은지 나쁜지를 알 수 있습니다.

395
00:40:42,821 --> 00:40:54,452
우리가 지금까지 배운 variance reduction 방법을
통상적으로 "vanilla REINFORCE" 라고 부릅니다.

396
00:40:54,452 --> 00:41:00,954
할인율을 적용하여 미래에 받을 보상을 누적시키고
여기에 단순한 baseline 을 추가하는 방식이죠

397
00:41:02,601 --> 00:41:08,769
자 그럼 이제는 baseline의 주요 아이디어와
더 좋은  baseline을 선택하는 방법에 대해서 살펴보겠습니다.

398
00:41:08,769 --> 00:41:13,567
우선 더 좋은 baseline이란 무엇인지 부터 생각해봅시다.

399
00:41:13,567 --> 00:41:24,255
우선 우리는 어떤 행동이 그 상태에서의 기댓 값 보다 좋은 경우에는
해당 상태에서 그 행동을 수행할  확률이 크길 원할 것입니다.

400
00:41:24,255 --> 00:41:30,163
자 그럼 그 "상태"로부터 기대할 수 있는 값이라면
여러분은 어떤 것이 생각나십니까?

401
00:41:30,163 --> 00:41:34,939
힌트는 우리가 앞서 한번 다룬 것입니다.

402
00:41:37,023 --> 00:41:41,297
[학생이 대답]
네 맞습니다. 가치 함수입니다.

403
00:41:41,297 --> 00:41:47,871
Q-learning에서 다뤘던 바로 그 가치함수입니다.
Q-function과 가치 함수를 이용해 볼 것입니다.

404
00:41:47,871 --> 00:41:58,173
여기에서의 직관은 우리가 어떤 상태 s에서 어떤 행동을 취했을때,
어떤 조건을 만족하면 그 행동이 좋았다고 판단하는 것입니다.

405
00:41:58,173 --> 00:42:04,752
그 조건은, 어떤 상태에서 특정 행동을
했을때 얻을 수 있는 Q-value가

406
00:42:04,752 --> 00:42:09,698
그 상태에서 얻을 수 있는 미래에 받을 수 있는 누적 보상들의
기댓 값이라는 가치함수 보다 더 큰 경우를 의미합니다.

407
00:42:09,698 --> 00:42:14,416
이는 그 행동이 우리가 선택하지 않은 다른
행동들보다 더 좋았다는 것을 의미합니다.

408
00:42:14,416 --> 00:42:22,063
반대로 그 차이 값이 음수거나 작은 경우에는
안 좋은 행동을 취했다는 것을 의미하는 것이겠죠

409
00:42:23,917 --> 00:42:34,868
이를 다시 estimator의 수식에 적용해 보겠습니다.

410
00:42:34,868 --> 00:42:40,168
수식 자체는 기존과 동일합니다.  다만

411
00:42:40,168 --> 00:42:50,514
앞선 수식에서는 variation reduction 기법과
baseline만 추가된 형태였다면, 지금은

412
00:42:50,514 --> 00:43:00,530
현재 행동이 얼마나 좋은 행동이었는지를 해당 상태에서의
Q-function 과 value function의 차이를 통해 나타냅니다.

413
00:43:01,771 --> 00:43:09,413
하지만 우리가 지금까지 살펴본 REINFORCE 알고리즘에서는
Q-function과 Value-function을 구하지 않았습니다.

414
00:43:09,413 --> 00:43:14,479
그렇다면 이를 학습시킬 수 있을까요?
정답은 '그렇다' 입니다. Q-learning을 통해서 말입니다.

415
00:43:14,479 --> 00:43:16,465
Q-learning은 이미 앞서 다뤘던 내용입니다.

416
00:43:16,465 --> 00:43:22,210
우리는 policy gradient와 Q-learning을 조합해서
이를 학습시킬 수 있습니다.

417
00:43:22,210 --> 00:43:28,784
여기에서는 actor 가 policy이고
critic이 Q-function 입니다.

418
00:43:28,784 --> 00:43:34,380
이들은 어떤 상태가 얼마나 좋은지, 그리고 상태에서의
어떤 행동이 얼마나 좋았는지를 말해줍니다.

419
00:43:34,380 --> 00:43:40,633
Actor-Critic 알고리즘에서
actor는 어떤 행동을 취할지를 결정합니다.

420
00:43:40,633 --> 00:43:47,708
그리고 critic (Q-function)은 그 행동이 얼마나 좋았으며
어떤 식으로 조절해 나가야하는지를 알려줍니다.

421
00:43:47,708 --> 00:43:53,636
그리고 critic의 Q-learning은 기존의 방법보다는
조금 완화된 임무를 수행합니다.

422
00:43:53,636 --> 00:43:59,958
기존의 Q-learning 에서는 모든 (상태, 행동) 쌍에 대한
Q-value 를 학습해야만 했습니다.

423
00:43:59,958 --> 00:44:04,762
하지만 여기에서는 policy가 만들어낸
(상태, 행동) 쌍에 대해서만 학습을 시키면  됩니다.

424
00:44:04,762 --> 00:44:10,512
계산이 필요한 곳에 어딘지만 알면 그만입니다.

425
00:44:10,512 --> 00:44:18,188
그리고 여기에서도 Q-function을 학습시킬 때, experience
replay와 같은 다양한 학습 전략을 추가할 수 있습니다.

426
00:44:18,188 --> 00:44:24,610
그리고 잠시 앞서 다뤘던 수식들을
새롭게 정의하고 넘어가겠습니다.

427
00:44:24,610 --> 00:44:38,172
이제는 Q(s, a) - V(s) 를 보상함수
(advantage function) 으로 나타낼 것입니다.

428
00:44:38,172 --> 00:44:43,568
즉 보상함수는 어떤 행동을 했을때 얼마나 많은
보상이 주어지는지를 나타냅니다.

429
00:44:43,568 --> 00:44:48,100
행동이 예상했던 것 보다 얼마나 더 좋은지를 나타내는 것이죠

430
00:44:48,100 --> 00:44:53,457
자 그럼 이를 이용해서 actor-critic 전체
알고리즘을 나타내 볼 수 있을 것입니다.

431
00:44:53,457 --> 00:44:56,279
자 그럼 어떻게 진행하는지를 살펴봅시다.

432
00:44:56,279 --> 00:45:03,689
우선은 policy parameter theta와
critic parameter phi를 초기화시킵니다.

433
00:45:03,689 --> 00:45:11,306
그리고 매 학습 interation 마다 현재의 정책을 기반으로
 M개의 경로를 샘플링합니다.

434
00:45:12,185 --> 00:45:18,725
policy에 기반해서 경로인 (s_0, a_0, r_0, s_1 ...)
를 뽑아내는 것입니다.

435
00:45:18,725 --> 00:45:21,671
그 다음 단계는 그레디언트를 계산하는 단계입니다.

436
00:45:21,671 --> 00:45:33,465
각 경로마다 보상 함수를 계산할 것입니다.
그리고 그 보상함수를 이용합니다.

437
00:45:33,465 --> 00:45:42,894
보상함수를 이용해서 gradient estimator를 계산하고
이를 전부 누적시킵니다.

438
00:45:42,894 --> 00:45:50,837
우리는 또한 critic parameter인
phi 또한 학습시켜야 합니다.

439
00:45:50,837 --> 00:45:57,557
이를 위해서는 가치 함수를 학습시켜야 하는데

440
00:45:57,557 --> 00:46:10,347
이는 보상 함수를 최소화 시키는 것과 동치(equivalent) 입니다.
이를 통해 가치 함수가 벨만 방정식에 근사하도록 학습시킬 수 있습니다.

441
00:46:10,347 --> 00:46:19,361
이런 식으로 poliy function(actor)과
critic function을 반복적으로 학습시킵니다.

442
00:46:20,211 --> 00:46:26,727
반복적으로 그레디언트를 업데이트하면서 학습을 진행합니다.

443
00:46:29,271 --> 00:46:31,827
자 그럼 REINFORCE를 활용한 몇 가지 예제를 살펴보겠습니다.

444
00:46:31,827 --> 00:46:39,464
우선은 Recurrent Attention Model (RAM)이 있습니다.

445
00:46:39,464 --> 00:46:49,146
hard attention 와도 관련이 있는 방법인데
요즘 컴퓨터 비전 분야에서 아주 핫합니다.

446
00:46:49,146 --> 00:46:59,167
우선 hard attention에 대해서 먼저 말씀드리겠습니다.
image classification 과 관련이 있는데

447
00:46:59,167 --> 00:47:06,494
여기에서도 이미지의 클래스를 분류하는 문제이긴 하지만
이미지의 일련의 glimpses를 가지고만 예측해야 합니다.

448
00:47:06,494 --> 00:47:10,300
이미지 전체가 아닌 지역적인 부분만을 보는 것이며

449
00:47:10,300 --> 00:47:17,141
어떤 부분을 볼지를 선택적으로 집중할 수 있으며
그렇게 살펴보면서 정보를 얻어나가는 것입니다.

450
00:47:17,141 --> 00:47:24,551
이런 식의 접근을 하는 이유는 우선 인간의 안구 운동에 따른
지각 능력으로부터의 영감이 있습니다.

451
00:47:24,551 --> 00:47:29,225
가령 우리가 복잡한 이미지를 보고 있고 이 이미지가
어떤 이미지인가를 알아내야 할 때

452
00:47:29,225 --> 00:47:39,168
처음에는 저해상도(low-resolution)로 살펴본 다음에
점점 더 세부적인 것들을 들여다 보게 되겠죠

453
00:47:39,168 --> 00:47:49,276
두번째로, 이런 식으로 이미지의 지역적인 부분만 살펴보는 방법은
계산 자원(computational resources)를 절약할 수 있습니다.

454
00:47:50,435 --> 00:47:53,293
이미지 전체를 전부 처리할 필요가 없겠죠

455
00:47:53,293 --> 00:48:03,576
우선 저해상도 살펴보면서 어디서부터 시작할 지 정하고
고해상도로 세부적인 것들을 찾아내는 방식이기대문입니다.

456
00:48:04,671 --> 00:48:15,177
이로 인해 계산 자원을 절약할 수 있으며, 크기가 큰 이미지들을
더 효과적으로 처리할 수 있는 scalability을 가질 수 있습니다.

457
00:48:16,164 --> 00:48:20,099
그리고 마지막으로 이 방법은 실제로
classification 성능을 높혀주기도 합니다.

458
00:48:20,099 --> 00:48:24,760
이 방법을 사용하면 필요없는 부분은 무시할 수 있기 때문입니다.

459
00:48:24,760 --> 00:48:29,931
ConvNet에 이미지 전체를 집어 넣는 것이 아니라

460
00:48:29,931 --> 00:48:36,353
ConvNet으로 실제로 내가 처리하고 싶은 부분만을
잘 선택할 수 있을 것입니다.

461
00:48:37,237 --> 00:48:41,531
그렇다면 이 문제를
강화학습 수식으로 나타내 보겠습니다.

462
00:48:41,531 --> 00:48:51,117
우선 상태(state)는 지금까지 관찰한 glimpses 입니다.
우리가 지금까지 얻어낸 정보라고 할 수도 있겠죠

463
00:48:51,117 --> 00:48:55,228
행동(action)은 다음에 이미지 내에 어떤
부분을 볼지를 선택하는 것입니다.

464
00:48:55,228 --> 00:49:02,842
실제로는 다음 스텝에서 보고 싶은 고정된 사이즈의
glimpse의 중간 x-y 좌표가 될 수 있습니다.

465
00:49:02,842 --> 00:49:12,423
강화학습에서 분류 문제를 풀때 보상은 최종스텝에서 1인데
이미지가 올바르게 분류되면 1, 그렇지 않으면 0입니다.

466
00:49:14,495 --> 00:49:24,550
이 문제에서 강화학습이 필요한 이유는 이미지에서 glimpses를
뽑아내는 것은 미분이 불가능한 연산이기 때문입니다.

467
00:49:25,761 --> 00:49:31,792
어떻게 glimpse를 얻어낼 것인지를 정책을 통해 학습하는 것입니다.
REINFORCE를 통해서 학습시킬 수 있습니다.

468
00:49:31,792 --> 00:49:40,891
누적된 glimpses가 모델에 주어지고
여기에서는 상태를 모델링하기 위해서 RNN을 이용합니다.

469
00:49:40,891 --> 00:49:47,418
그리고 policy parameters를 이용해서
다음 액션을 선택하게 됩니다.

470
00:49:49,354 --> 00:49:54,571
자 그럼 모델이 어떻게 생겼는지 살펴봅시다.
우선 입력 이미지가 들어옵니다.

471
00:49:54,571 --> 00:50:03,184
그리고 이미지에서 glimpse를 추출합니다. 여기에서는
여기 빨간색 박스가 glimpse인데 이 경우에는 비어있습니다.

472
00:50:03,184 --> 00:50:12,193
그리고 추출된 glimpse는 neural network를 통과합니다.
모델은 테스크에 따라 어떤 모델이든 가능합니다.

473
00:50:12,193 --> 00:50:18,758
논문의 실험에서는 MNIST를 사용했는데, MNIST는 단순한
데이터셋이라서 몇 개의 작은 FC-layers로 충분할 것입니다.

474
00:50:18,758 --> 00:50:26,105
만일 조금 더 복잡한 이미지 데이터셋이 있다면
fancier ConvNets을 고려하는 것도 좋은 방법입니다.

475
00:50:26,105 --> 00:50:28,775
glimpse를 neural entwork에 통과시키고 나면

476
00:50:28,775 --> 00:50:36,115
앞서 말씀드렸듯이 RNN을 이용해서 지금까지 있었던
glimpses를(state) 전부 결합시켜 줘야 합니다.

477
00:50:36,115 --> 00:50:40,265
이 과정은 잠시 후에 다시한번 알아보겠습니다.

478
00:50:40,265 --> 00:50:46,094
Neural network의 출력은 x-y 좌표 입니다.

479
00:50:46,094 --> 00:50:50,766
실제로는 출력 값이 행동에 대한 분포의 형태인데

480
00:50:50,766 --> 00:50:57,282
이 분포는 가우시안 분포를 따르며
결국 출력 값은 분포의 평균이 될 것입니다.

481
00:50:57,282 --> 00:51:03,944
평균과 분산을 모두 출력하는 경우도 있고
분산은 고정된 값으로 설정하기도 합니다.

482
00:51:03,944 --> 00:51:11,854
자 그럼 이 행동 분포로부터 특정 x, y
위치를 샘플링한 다음에

483
00:51:11,854 --> 00:51:17,777
이 x-y 좌표를 이용해서 다음 glimpse 를 얻어냅니다.

484
00:51:17,777 --> 00:51:23,385
새로운 glimpse를 보시면 숫자 2의 꼬리 부분으로
이동할 것을 볼 수 있습니다.

485
00:51:23,385 --> 00:51:26,745
이제야 비로서 모델이 우리가 원하는 부분을
보기 시작한 것 같습니다.

486
00:51:26,745 --> 00:51:32,724
우리가 원하던건 모델이 이미지에서 classification에
유용한 부분을 보길 원했으니까요

487
00:51:32,724 --> 00:51:37,104
그리고 다시 새롭게 추출한 glimpse를
neural network에 통과시킵니다.

488
00:51:38,153 --> 00:51:47,343
그리고 현재 입력과 이전 상태(previous hidden state)
를 입력으로 받는 RNN을 이용해서 policy를 모델링 할 것입니다.

489
00:51:47,343 --> 00:51:54,095
이 RNN 모델은 다음 위치의 glimpse에 대한 분포를 출합니다.

490
00:51:54,095 --> 00:51:57,303
이 작업을 반복합니다.
다음 glimpse는 여기군요

491
00:51:57,303 --> 00:51:59,903
기존보다 숫자 2의 중앙으로 조금 더 움직였습니다.

492
00:51:59,903 --> 00:52:14,543
아마 이 모델은 숫자 2의 꼬리 부분을 봤으면 조금 더 의미있는
정보를 얻기 위해 좌상단으로 이동하도록 학습된 것 같습니다.

493
00:52:14,543 --> 00:52:17,473
이를 계속 반복해줍니다.

494
00:52:17,473 --> 00:52:26,795
그리고 마지막 타입 스텝에 도달하게 됩니다. 타임스텝의 수는
고정된 값을 주로 사용합니다. 실제로는 6-8번 정도를 사용합니다.

495
00:52:26,795 --> 00:52:33,350
그리고 마지막 타임 스텝에서는, 결국 우리는
분류를 하고싶기 때문에

496
00:52:33,350 --> 00:52:39,363
Softmax를 통해서 각 클래스 확률분포를
출력하도록 합니다.

497
00:52:39,363 --> 00:52:44,108
가령 이 경우에는 가장 높은 확률의 클래스가 2이고
결국 모델이 숫자를 2라고 잘 예측할 수 있었습니다.

498
00:52:44,108 --> 00:52:50,428
자 여기까지는 모델과 정책이 어떻게 이루어졌는지를 살펴보있습니다.

499
00:52:50,428 --> 00:52:59,569
그레디언트를 구하는 법은 기존과 동일합니다. 이 모델로부터
경로를 샘플링하고 그레디언트를 계산해서 backprob를 진행합니다.

500
00:52:59,569 --> 00:53:08,698
REINFORCE 알고리즘을 통해서 모델과
정책 파라미터를 학습시킬 수 있을 것입니다.

501
00:53:09,953 --> 00:53:15,257
MNIST로 학습시킨 예시입니다.

502
00:53:16,710 --> 00:53:27,685
결과를 보시면 어느 지점에서 시작하던지 점점 숫자의 위치에
가까워지는 것을 보실 수 있습니다. 아주 멋진 결과입니다.

503
00:53:27,685 --> 00:53:30,460
우리가 기대하던 바입니다.

504
00:53:30,460 --> 00:53:38,400
만일 여러분이 어떤 숫자인지 알기 위해 다음에 볼 가장
효과적인 곳을 고른다고 해도 비슷할 것입니다.

505
00:53:40,108 --> 00:53:49,758
Hard attention이라는 아이디어는 지난 몇 년간
다양한 컴퓨터문제를 푸는데 사용되었습니다.

506
00:53:49,758 --> 00:53:54,790
가령 fine-grained image recognition
같은 문제에 사용할 수 있습니다.

507
00:53:54,790 --> 00:54:01,763
그리고 앞서 말씀드렸듯이 이 방법의 이점으로는

508
00:54:02,975 --> 00:54:10,180
계산 효율이 좋다는 점과 이미지 내의
불필요한 정보들을 무시할 수 있다는 점입니다.

509
00:54:10,180 --> 00:54:13,092
만일 여러분이 fine-grained image classification
문제를 풀고자 한다면 이 두마리 토끼를 다 잡고 싶을 것입니다.

510
00:54:13,092 --> 00:54:25,777
고해상도를 유지하여 중요한 특징들을 잘 보존하면서도
불필요한 부분들은 무시하고 싶은 그런 상황일때 말이죠

511
00:54:25,777 --> 00:54:27,359
질문있나요?

512
00:54:27,359 --> 00:54:31,526
[학생이 질문]

513
00:54:35,061 --> 00:54:41,482
질문은 "RNN 모델이 들어가는데, 계산이
효율적일 수 있는지" 입니다.

514
00:54:41,482 --> 00:54:47,761
네 맞습니다. 사실 계산 효율과 관련된 문제는 여러분이
어떤 문제를 풀고 어떤 네트워크를 선택했는지 등에 달려있습니다.

515
00:54:47,761 --> 00:54:52,512
가령 여러분이 아주 고해상도의 이미지가 있다고 해봅시다.

516
00:54:52,512 --> 00:55:01,900
여러분은 이 이미지 전체를 처리하고 싶지 않을 수도 있습니다.
이 이미지를 처리하려면 아주 큰 네트워크가 필요할 수도 있습니다.

517
00:55:01,900 --> 00:55:04,669
이미지의 특정 일부분에만 집중한다면
어느정도 계산량을 절약할 수 있을 것입니다.

518
00:55:04,669 --> 00:55:06,589
이미지 일부만 처리한다면 말이죠

519
00:55:06,589 --> 00:55:10,924
그렇긴 하지만 결국은 어떤 문제냐에 달려있습니다.

520
00:55:12,210 --> 00:55:14,530
REINFORCE 알고리즘은
이미지 캡셔닝에도 사용할 수 있습니다.

521
00:55:14,530 --> 00:55:17,138
어떤 이미지에 대한 캡션을 만드는 상황에서

522
00:55:17,138 --> 00:55:23,197
이런 류의 attention model을 이미지에 사용할 수 있을 것입니다.

523
00:55:23,197 --> 00:55:28,999
정책을 이미지의 특정 부분에 집중하도록 학습시키고

524
00:55:28,999 --> 00:55:38,341
이미지의 특정 부분에 집중하면서 그 부분에 적절한
캡션을 생성할 수 있도록 하는 방법입니다.

525
00:55:38,341 --> 00:55:42,948
그리고 visual question answering(VQA)
문제를 풀어볼 수도 있습니다.

526
00:55:42,948 --> 00:55:51,786
이미지에 대해 질문을 하면 모델은 적절한 답변을 해야 합니다.
저는 잘 모르지만, 가령

527
00:55:51,786 --> 00:55:53,840
테이블 주변에 의자가 몇 개 있습니까? 와 같은 질문들입니다.

528
00:55:53,840 --> 00:56:03,040
attention mechanism을 이용하면 그러한 모델이 그러한 
질문에 잘 답변할 수 있도록 학습시킬 수 있을 것입니다.

529
00:56:05,707 --> 00:56:10,564
지금까지는 policy gradient를 활용한 
hard attention models의 예시를 살펴봤습니다.

530
00:56:10,564 --> 00:56:16,430
예시를 하나 더 살펴볼 것입니다. 
이 또한 policy gradient를 이용한 방법입니다.

531
00:56:16,430 --> 00:56:18,465
바둑을 학습시키는 모델입니다.

532
00:56:18,465 --> 00:56:24,497
DeepMind에는 AlphaGo 라고 하는 
바둑을 두는 에이전트가 있습니다.

533
00:56:24,497 --> 00:56:30,708
몇 해 전부터 뉴스에도 아주 많이 나왔습니다.

534
00:56:30,708 --> 00:56:31,541
질문있나요?

535
00:56:31,541 --> 00:56:32,374
[학생이 질문]

536
00:56:32,374 --> 00:56:39,258
네 어제에도 (뉴스가) 있었습니다.
아주 흥미로운 뉴스였죠

537
00:56:39,258 --> 00:56:57,886
지난 해, AlphaGo v1과 이세돌과의 대국이 있었습니다.
5번의 대국 중 4:1로 AlphaGo가 승리했었죠

538
00:56:57,886 --> 00:57:09,348
현재 AlphaGo는 커제(Ke Jie)와의 
3선승제 대국을 진행중입니다.

539
00:57:09,348 --> 00:57:13,436
첫 대국이 어제 있었고
AlphaGo가 승리했습니다.

540
00:57:13,436 --> 00:57:20,657
첫 대국에서는 AlphaGo가 0.5점 앞서 승리를 거두었습니다. 
두 경기가 남았고, 라이브 스트리밍을 지원하니

541
00:57:20,657 --> 00:57:28,193
온라인으로 시청해 보시는 것도 좋을 것입니다. 
해설을 들으면서 보면 상당히 재밌습니다.

542
00:57:29,225 --> 00:57:32,577
그러면 DeepMind의 AlphaGo는 무엇일까요?

543
00:57:32,577 --> 00:57:36,466
AlphaGo의 많은 부분은 우리가 이번 시간에 
다룬 것들을 기반으로 합니다.

544
00:57:36,466 --> 00:57:42,045
AlphaGo에는 지도학습과 강화학습이 섞여있습니다.

545
00:57:42,045 --> 00:57:51,656
또한 Monte Carlo Tree Search와 같은 오래된 방식과
아주 최신의 deep RL 방법도 섞여있습니다.

546
00:57:52,579 --> 00:57:56,986
AlphaGo가 어떻게 세계 챔피언을 이길 수 있었을까요?

547
00:57:56,986 --> 00:58:08,739
우선 입력 벡터를 만들어야 합니다. 우선 바둑판과 바둑 돌의 위치와
같은 요소들을 특징화 시켜 AlpaGo의 입력이 될 수 있도록 합니다.

548
00:58:08,739 --> 00:58:10,739
이 과정은 상태를 설계하는 아주 자연스러운 단계입니다.

549
00:58:10,739 --> 00:58:17,958
AlphaGo는 성능 향상을 위해 몇 가지 
채널을 조금 더 추가했습니다.

550
00:58:17,958 --> 00:58:23,790
바둑 돌의 색깔에 따라 채널을 따로 만들기도 했습니다.
이는 바둑판의 형세(configuration)를 표현하는 역할을 합니다.

551
00:58:23,790 --> 00:58:31,138
그리고 이외에도 몇 가지 채널이 더 추가되었는데 가령
move ligality, bias 채널 등 다양한 채널이 추가되었습니다.

552
00:58:31,138 --> 00:58:36,518
이렇게 상태를 정의하고 나면
네트워크를 학습시킬 차례입니다.

553
00:58:36,518 --> 00:58:40,897
네트워크는 프로 바둑기사의 기보를 지도학습으로
학습시켜서 초기화합니다.

554
00:58:40,897 --> 00:58:48,495
바둑판의 현재 상태가 주어지면 
다름 행동을 어떻게 취할지를 결정합니다.

555
00:58:48,495 --> 00:58:55,608
프로 바둑기사의 기보가 주어지면

556
00:58:55,608 --> 00:59:02,605
프로 바둑기사가 두는 수들을
supervised mapping으로  학습시킵니다.

557
00:59:02,605 --> 00:59:05,365
이것이 DeepMind가 AlphaGo를 초기화시켰던 방법이고
상당히 좋은 방법입니다.

558
00:59:05,365 --> 00:59:09,227
다음 단계는 policy network를 초기화할 차례입니다.

559
00:59:09,227 --> 00:59:17,778
pilicy network는 바둑판의 상태를 입력으로 받아서
어떤 수를 둬야하는지를 반환해 줍니다.

560
00:59:17,778 --> 00:59:21,978
지금까지 살펴본 policy gradients가
거의 다 이런식으로 동작했었죠

561
00:59:21,978 --> 00:59:27,130
이 또한 policy gradients를 이용해서
지속적으로 학습시킵니다.

562
00:59:27,130 --> 00:59:35,123
그리고 AlphaGo는 임의의 이전 interation에서의 
자기 자신과 대국을 두며 학습을 진행합니다.

563
00:59:35,123 --> 00:59:42,243
스스로 대국을 둬서 이기면 보상을 받고 (1) 
지면 -1의 보상을 받습니다.

564
00:59:42,243 --> 00:59:47,573
그리고 value network도 학습을 해야 합니다.
앞서 critic network로 살펴봤었죠

565
00:59:47,573 --> 01:00:01,475
최종적인 AlphaGo는 policy/value network와
Monte Carlo Tree Search 알고리즘의 결합된 형태입니다.

566
01:00:01,475 --> 01:00:19,891
AlphaGo가 다음 수를 어디에 놓을지는 value function
과 MCTS로 계손된 값의 조합으로 결정합니다.

567
01:00:19,891 --> 01:00:27,453
지금까지 AlphaGo의 기본적인 구성요소를 설명해 드렸습니다.

568
01:00:28,397 --> 01:00:33,814
더 관심있으신 분들은 2016 nature에 게제된
AlphaGo 논문을 참고하시기 바랍니다.

569
01:00:34,664 --> 01:00:47,953
지금 대국에 사용하는 AlphaGo는 제 생각에 
수천 개의 CPU와 수 백개의 GPU를 이용하고 있습니다.

570
01:00:47,953 --> 01:00:52,120
아주 엄청난 량의 학습이 필요할 것입니다.

571
01:00:55,659 --> 01:01:01,529
그리고 이번 주에 있는 경기는 한 번 챙겨보시기 
바랍니다. 아주 재미있습니다.

572
01:01:03,491 --> 01:01:10,858
오늘 한 내용을 요약해봅시다. 오늘 우리는
policy gradients를 배웠습니다. 아주 범용적인 방법입니다.

573
01:01:10,858 --> 01:01:18,855
gradient descent/ascent를 통해 
policy parameters를 직접 업데이트했습니다.

574
01:01:18,855 --> 01:01:23,947
많은 경우에 잘 동작하긴 하지만
high-variance 문제가 있었습니다.

575
01:01:23,947 --> 01:01:28,669
이로 인해 아주 많은 샘플이 필요했고 
sample-efficiency에 관한 문제가 대두되었습니다.

576
01:01:28,669 --> 01:01:35,349
우리는 Q-learning에 대해서도 배웠습니다. 
이 방법은 항상 잘 동작하지는 않습니다.

577
01:01:35,349 --> 01:01:46,730
왜냐하면 매우 큰 고차원 공간에서 
(상태, 행동) 쌍을 전부 계산해야 하기 때문입니다.

578
01:01:47,769 --> 01:01:54,322
하지만 Atari의 예 처럼 잘 동작하기만 하면 
policy gradients보다는 sample-efficient 합니다.

579
01:01:54,322 --> 01:01:59,902
Q-learning에서 가장 중요한 것은
중분한 exploration을 해야한다는 것입니다.

580
01:01:59,902 --> 01:02:04,902
질문있나요?
[학생이 질문]

581
01:02:14,313 --> 01:02:21,753
질문은 supervised training을 
Q-learning에 적용해볼 수 있는지 입니다.

582
01:02:21,753 --> 01:02:24,924
제 생각에는 Q-learning에 직접 적용하는건
힘들 것 같습니다.

583
01:02:24,924 --> 01:02:27,532
Q-learning은 Q-values를 regression 하는
과정이기 때문입니다.

584
01:02:27,532 --> 01:02:46,021
반명 policy gradient는 분포를 다루고 있죠. 그렇긴 해도 다룰 수
있는 방법이 있을 것입니다. 가령 bootstrap 등을 사용하면 말이죠

585
01:02:47,454 --> 01:02:54,213
이번 시간에 policy gradients와 Q-learning을 다뤘습니다.

586
01:02:54,213 --> 01:02:56,752
Policy gradient는 어느정도 보장성을 지니고 있습니다.

587
01:02:56,752 --> 01:03:02,789
Policy gradient는 항상 J(theta)의 local minima로
수렴할 수 있고, 이는 아주 좋은 특성입니다.

588
01:03:04,339 --> 01:03:12,131
왜냐하면 Policy gradient는 gradient ascent를 직접
수행하기 때문이며, local minima도 제법 괜찮은 편이기 때문이죠

589
01:03:12,131 --> 01:03:17,041
반면 Q-learning의 경우에는 어떠한 보장도 없습니다. 
왜냐하면 Q-learning은

590
01:03:17,041 --> 01:03:23,358
함수 근사를 통해 벨만 방정식을 근사시키는 것이기 때문입니다.

591
01:03:23,358 --> 01:03:29,954
이로 인해 다양한 문제에 응용 가능성을 고려해보면
Q-learning은 학습시키기 매우 까다롭다고 할 수 있습니다.

592
01:03:31,849 --> 01:03:41,546
오늘은 강화학습에 대해 아주 간략한 개요와
주요 알고리즘들을 몇 가지 살펴보았습니다.

593
01:03:41,546 --> 01:03:47,577
다음 시간에는 Song Han의 guest lecture가 있습니다.

594
01:03:47,577 --> 01:03:52,569
Song Han은 모델 압축(model compression)과 
energy efficient deep learning의 선구자입니다.

595
01:03:52,569 --> 01:03:56,459
Song Han과 함께 이에 대해서 한번 알아보도록 하겠습니다.

596
01:03:56,459 --> 01:03:58,459
감사합니다!

