{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準備完了\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "print(\"準備完了\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データを取るためにclass生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성\n"
     ]
    }
   ],
   "source": [
    "class ThubanNaver:\n",
    "\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    index_to_word, word_to_index = {}, {}\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"생성\")\n",
    "        \n",
    "    def set_init_words(self):\n",
    "        # set word dictionary\n",
    "        index_to_word = pd.read_csv('words.csv')\n",
    "        self.index_to_word = {idx:word for idx, word in index_to_word['word'].items()}\n",
    "        self.word_to_index = {word:idx for idx, word in self.index_to_word.items()}\n",
    "        \n",
    "    def set_init_datas(self):\n",
    "        read_train = pd.read_csv('train.csv').dropna()\n",
    "        read_test = pd.read_csv('test.csv').dropna()\n",
    "        \n",
    "        self.X_train = [self.get_encoded_sentence(text) for idx, text in read_train['text'].items()]\n",
    "        self.X_test = [self.get_encoded_sentence(text) for idx, text in read_test['text'].items()]\n",
    "        \n",
    "        self.y_train = np.array([label for idx, label in read_train['label'].items()])\n",
    "        self.y_test = np.array([label for idx, label in read_test['label'].items()])\n",
    "        \n",
    "    def save_datas_to_csv(self):\n",
    "        \n",
    "        # 데이터를 읽어봅시다. \n",
    "        train_txt = pd.read_table('ratings_train.txt')\n",
    "        test_txt = pd.read_table('ratings_test.txt')\n",
    "        # 重複のデータを削除, そしてNANを削除\n",
    "        train_txt = train_txt.drop_duplicates('document').dropna()\n",
    "        test_txt = test_txt.drop_duplicates('document').dropna()\n",
    "        \n",
    "        \n",
    "        okt = Okt()\n",
    "        \n",
    "        stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "        \n",
    "        #セーブされる配列\n",
    "        train_texts = []\n",
    "        test_texts = []\n",
    "        clean_words = []\n",
    "        \n",
    "\n",
    "        # 単語辞書をcsvファイルにセーブために韓国語だけまたはtoken化する\n",
    "        for i in range(0,2):\n",
    "            if i == 0:\n",
    "                read_x_datas = train_txt['document']\n",
    "                save_x_datas = train_texts\n",
    "            else:\n",
    "                read_x_datas = test_txt['document']\n",
    "                save_x_datas = test_texts\n",
    "                \n",
    "            for idx, item in enumerate(read_x_datas.items()):\n",
    "                item_text = re.compile('[가-힣]+').findall(item[1])\n",
    "                results = okt.morphs(' '.join(item_text))\n",
    "                stopwords_result = []\n",
    "                for result in results:\n",
    "                    if result not in stopwords: #単語たちのstopwordsを削除\n",
    "                        stopwords_result.append(result)\n",
    "                \n",
    "                # stopwordsしても文字がある場合だけ\n",
    "                if(len(stopwords_result) > 0):\n",
    "                    save_x_datas.append(' '.join(stopwords_result)) # trainまたはtestデータ\n",
    "                    clean_words.extend(stopwords_result) #辞書のcsvのため       \n",
    "                # print(\"{} {}\".format(results, idx))\n",
    "                \n",
    "                # 全部forしたら時間がかかるので（テスト用）\n",
    "                #if idx == 99:\n",
    "                #    break\n",
    "\n",
    "        #単語たちの重複を削除\n",
    "        clean_words = set(clean_words)\n",
    "        clean_words = list(clean_words)\n",
    "        \n",
    "        #\n",
    "        clean_words.insert(0, \"<UNUSED>\")\n",
    "        clean_words.insert(0, \"<UNK>\")\n",
    "        clean_words.insert(0, \"<BOS>\")\n",
    "        clean_words.insert(0, \"<PAD>\")\n",
    "        \n",
    "        \n",
    "        # words.csvセーブ\n",
    "        pd.DataFrame(clean_words, columns=['word']).to_csv('words.csv', encoding=\"utf-8\")\n",
    "        self.set_init_words() #下のtexts, labelsをセーブするため (classの変数 index_to_word, word_to_indexに 実際の値を入力)\n",
    "        \n",
    "        \n",
    "        # trainとtestをcsvでセーブする\n",
    "        train_labels = [result for result in train_txt['label']][:len(train_texts)]\n",
    "        test_labels = [result for result in test_txt['label']][:len(test_texts)]\n",
    "\n",
    "        train_df = pd.DataFrame({'text':train_texts, 'label':train_labels})\n",
    "        test_df = pd.DataFrame({'text':test_texts, 'label':test_labels})\n",
    "        \n",
    "        pd.DataFrame(train_df).to_csv('train.csv', encoding=\"utf-8\")\n",
    "        pd.DataFrame(test_df).to_csv('test.csv', encoding=\"utf-8\")\n",
    "        self.set_init_datas() # classの変数 X_train, y_train, X_test, y_testに 実際の値を入力\n",
    "        \n",
    "        \n",
    "    def load_csv_datas(self):\n",
    "        self.set_init_words()\n",
    "        self.set_init_datas()\n",
    "        \n",
    "    def load_data(self, num_words):\n",
    "        if len(self.word_to_index.items()) < num_words: #num_words数がword_to_index配列の長さより長い場合はword_to_indexの長さに従う\n",
    "            num_words = len(self.word_to_index.items())\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test, [ item for item, idx in self.word_to_index.items() if idx < int(num_words) ]\n",
    "        \n",
    "        \n",
    "    # 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "    # 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "    def get_encoded_sentence(self, sentence):\n",
    "        return [self.word_to_index['<BOS>']]+[self.word_to_index[word] if word in self.word_to_index else self.word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "    # 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "    def get_encoded_sentences(self, sentences):\n",
    "        return [self.get_encoded_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    # 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentence(self, encoded_sentence):\n",
    "        return ' '.join(self.index_to_word[index] if index in self.index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "    # 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentences(self, encoded_sentences):\n",
    "        return [self.get_decoded_sentence(encoded_sentence, self.index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "TN = ThubanNaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ratings_train.txt'と'ratings_test.txt'だけで   \n",
    "X_train, y_train, X_test, y_test, words_to_indexの値を得るためのclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"スタート\")\n",
    "TN.save_datas_to_csv()\n",
    "print(\"データセーブ、ロード完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データロード完了\n"
     ]
    }
   ],
   "source": [
    "# すでにTN.save_datas_to_csv()をして words.csv, x_test.csv, x_train.csv ファイルが居る場合は下の関数を利用してデータを読み込みする\n",
    "TN.set_init_words()\n",
    "TN.set_init_datas()\n",
    "print(\"データロード完了\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを初めて扱っているならTN.save_datas_to_csv()関数を   \n",
    "そうじゃなければなければ   \n",
    "TN.set_init_words()   \n",
    "TN.set_init_datas()   \n",
    "を使ってください   \n",
    "   \n",
    "### こうやって二つのコードに割った理由は TN.save_datas_to_csv() この関数が結構時間がかかるため"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 145237, 테스트 개수: 48773\n",
      "単語数: 113661\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, words_to_index = TN.load_data(num_words=113661)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(X_train), len(X_test)))\n",
    "print(\"単語数: {}\".format(len(words_to_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_data(num_words=n)のnum_wordsで単語を何個持ってくるのかを決めます   \n",
    "存在する単語数より多い場合は存在する単語すべてを持ってきます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          1818576   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,822,337\n",
      "Trainable params: 1,822,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 113661    # 어휘 사전의 크기입니다\n",
    "word_vector_dim = 16 # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 설계. 위에서 만든 모델을 사용해봅시다.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None, )))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) # 최종 출력은 긍정/부정을 나타내는 1dim\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配列の長さをお互い同じくする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  12.058857790835525\n",
      "문장길이 최대 :  73\n",
      "문장길이 표준편차 :  9.270684715127398\n",
      "pad_sequences maxlen :  30\n",
      "전체 문장의 0.9344982217411474%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配列の長さをお互い同じくする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=TN.word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=TN.word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "\n",
    "X_train = embedding(X_train)\n",
    "X_test = embedding(X_test)\n",
    "\n",
    "print(\"clear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配列の長さをお互い同じくしなければ   \n",
    "モデルの訓練ができなくなります   \n",
    "   \n",
    "そしてpad_sequencesの関数の中でpadding='pre'はpreがいいらしい   \n",
    "X_train、X_testの配列一つ一つには文章があるんですが個々長さが違うので後ろのには空白が出ます   \n",
    "モデルの訓練の時空白が最後に計算して悪い影響が出るそうです   \n",
    "なので列の前の方に空白を置いて後ろは実際の文字を置くpreがいいらしいです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 16), dtype=float32, numpy=\n",
       "array([[ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [ 0.04613856,  0.01403167,  0.02923531, -0.02850889,  0.03368899,\n",
       "         0.0168766 ,  0.01934865, -0.04111039,  0.01275269,  0.04354442,\n",
       "         0.03784927, -0.00735014, -0.04301777,  0.04044653,  0.03388585,\n",
       "         0.02398138],\n",
       "       [-0.01362314, -0.02935108,  0.01342806,  0.03307367,  0.0183886 ,\n",
       "        -0.03953998,  0.04352161,  0.01794631,  0.014142  , -0.04179187,\n",
       "        -0.00997214,  0.04034625, -0.01113914,  0.02376249, -0.04476799,\n",
       "        -0.00376602],\n",
       "       [ 0.01068336,  0.03160462,  0.00113776,  0.04105712, -0.01223053,\n",
       "         0.00863925,  0.02564411,  0.01616896, -0.01631814,  0.00597787,\n",
       "        -0.02672355, -0.00627316, -0.03425062, -0.01972544,  0.01301451,\n",
       "        -0.04304105],\n",
       "       [ 0.0307469 , -0.01086922, -0.01441077, -0.01160693, -0.02320852,\n",
       "         0.03572677, -0.01929348,  0.03904948, -0.0148832 , -0.00808308,\n",
       "         0.03413003,  0.03321246, -0.04444443,  0.02183845, -0.03522157,\n",
       "         0.02845922],\n",
       "       [-0.01943022, -0.00678867,  0.00778899, -0.03755416,  0.03773919,\n",
       "         0.00304024,  0.01145359, -0.02118099, -0.04827685, -0.02483985,\n",
       "         0.00010062, -0.0373397 , -0.04869647,  0.03130068,  0.00037823,\n",
       "         0.00396347],\n",
       "       [-0.01022502,  0.01567239, -0.00169523,  0.03371953, -0.00405029,\n",
       "        -0.04244496,  0.03620731, -0.03390259,  0.02206788, -0.03902949,\n",
       "        -0.01344246,  0.00425044, -0.01108438, -0.03761643,  0.02568168,\n",
       "        -0.0413922 ],\n",
       "       [-0.0280233 ,  0.0045674 , -0.03949344,  0.0104714 , -0.03435763,\n",
       "         0.04895134, -0.03001993,  0.00537276, -0.01057435, -0.02922431,\n",
       "         0.01616805, -0.04543377,  0.03391185,  0.02194216,  0.0061156 ,\n",
       "         0.01634592]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの正確性を上げるためTrainingとValidationで割る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95237, 30, 16)\n",
      "(50000, 30, 16)\n"
     ]
    }
   ],
   "source": [
    "# validation set 50000건 분리\n",
    "x_val = X_train[:50000]   \n",
    "y_val = y_train[:50000]\n",
    "\n",
    "# validation set을 제외한 나머지\n",
    "partial_x_train = X_train[50000:]  \n",
    "partial_y_train = y_train[50000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingデータ95237列   \n",
    "Validationデータ50000列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([95237, 30, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input Tensor(\"embedding_1_input:0\", shape=(None, None), dtype=float32), but it was called on an input with incompatible shape (None, 30, 16).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:277 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:719 call\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:888 _run_internal_graph\n        output_tensors = layer(computed_tensors, **kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:886 __call__\n        self.name)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:180 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer conv1d_2 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 30, 16, 16]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5f0bf04378a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:277 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:719 call\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:888 _run_internal_graph\n        output_tensors = layer(computed_tensors, **kwargs)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:886 __call__\n        self.name)\n    /home/aiffel/Downloads/exit/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:180 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer conv1d_2 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 30, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
