{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準備完了\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "print(\"準備完了\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データを取るためにclass生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성\n"
     ]
    }
   ],
   "source": [
    "class ThubanNaver:\n",
    "\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    index_to_word, word_to_index = {}, {}\n",
    "    \n",
    "    # データをcsvファイルでセーブしたかい否か\n",
    "    data_save = False\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"생성\")\n",
    "    \n",
    "        \n",
    "    def set_init_words(self):\n",
    "        # set word dictionary\n",
    "        index_to_word = pd.read_csv('words.csv')\n",
    "        self.index_to_word = {idx:word for idx, word in index_to_word['word'].items()}\n",
    "        self.word_to_index = {word:idx for idx, word in self.index_to_word.items()}\n",
    "        \n",
    "    def set_init_datas(self):\n",
    "        read_train = pd.read_csv('train.csv')\n",
    "        read_test = pd.read_csv('test.csv')\n",
    "        \n",
    "        self.X_train = [self.get_encoded_sentence(text) for idx, text in read_train['text'].items()]\n",
    "        self.X_test = [self.get_encoded_sentence(text) for idx, text in read_test['text'].items()]\n",
    "        \n",
    "        self.y_train = np.array([label for idx, label in read_train['label'].items()])\n",
    "        self.y_test = np.array([label for idx, label in read_test['label'].items()])\n",
    "        \n",
    "    def save_datas_to_csv(self, num_sentences):\n",
    "        \n",
    "        # 데이터를 읽어봅시다. \n",
    "        train_txt = pd.read_table('ratings_train.txt')\n",
    "        test_txt = pd.read_table('ratings_test.txt')\n",
    "        \n",
    "        # documentの行で重複のデータを削除\n",
    "        train_txt.drop_duplicates(subset=['document'], inplace=True)\n",
    "        test_txt.drop_duplicates(subset = ['document'], inplace=True)\n",
    "        \n",
    "        # 行の中で一つの列でも NAN だったらその行を削除\n",
    "        train_txt = train_txt.dropna(how = 'any')\n",
    "        test_txt = test_txt.dropna(how='any') # Null 값 제거\n",
    "\n",
    "        okt = Okt()\n",
    "        \n",
    "        stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "        \n",
    "        #セーブされる配列\n",
    "        train_texts = []\n",
    "        test_texts = []\n",
    "        clean_words = []\n",
    "        train_labels = []\n",
    "        test_labels = []\n",
    "        \n",
    "\n",
    "        # 単語辞書をcsvファイルにセーブために韓国語だけまたはtoken化する\n",
    "        for i in range(0,2):\n",
    "            if i == 0:\n",
    "                read_x_datas = train_txt\n",
    "                save_x_datas = train_texts\n",
    "                save_y_datas = train_labels\n",
    "            else:\n",
    "                read_x_datas = test_txt\n",
    "                save_x_datas = test_texts\n",
    "                save_y_datas = test_labels\n",
    "                \n",
    "            num_sentences_check = 0\n",
    "            for key, item in read_x_datas['document'].items():\n",
    "                item_text = re.compile('[가-힣]+').findall(item)\n",
    "                results = okt.morphs(' '.join(item_text), stem=True)\n",
    "                stopwords_result = []\n",
    "                for result in results:\n",
    "                    if result not in stopwords: #単語が２文字以上だけ（僕なりの正確度を上げるため）, 単語たちのstopwordsを削除\n",
    "                        stopwords_result.append(result)\n",
    "                \n",
    "                # stopwordsしても文字がある場合だけ\n",
    "                if(len(stopwords_result) > 0):\n",
    "                    save_x_datas.append(' '.join(stopwords_result)) # trainまたはtestデータの文章\n",
    "                    save_y_datas.append(read_x_datas['label'][key])\n",
    "                    num_sentences_check += 1 # 指定された数ぐらいセーブするため\n",
    "                \n",
    "                    if i == 0: # trainデータ\n",
    "                        clean_words.extend(stopwords_result) #trainデータのみ辞書を作る    \n",
    "                        \n",
    "                \n",
    "                # 全部forしたら時間がかかるので（テスト用）\n",
    "                if num_sentences_check == num_sentences:\n",
    "                    break\n",
    "\n",
    "        #単語たちの重複を削除\n",
    "        clean_words = set(clean_words)\n",
    "        clean_words = list(clean_words)\n",
    "        \n",
    "        #\n",
    "        clean_words.insert(0, \"<UNK>\")\n",
    "        clean_words.insert(0, \"<PAD>\")\n",
    "\n",
    "\n",
    "        # words.csvセーブ\n",
    "        pd.DataFrame(clean_words, columns=['word']).to_csv('words.csv', encoding=\"utf-8\")\n",
    "        self.set_init_words() #下のtexts, labelsをセーブするため (classの変数 index_to_word, word_to_indexに 実際の値を入力)\n",
    "        \n",
    "        # trainとtestをcsvにセーブする\n",
    "        train_df = pd.DataFrame({'text':train_texts, 'label':train_labels})\n",
    "        test_df = pd.DataFrame({'text':test_texts, 'label':test_labels})\n",
    "        \n",
    "        pd.DataFrame(train_df).to_csv('train.csv', encoding=\"utf-8\")\n",
    "        pd.DataFrame(test_df).to_csv('test.csv', encoding=\"utf-8\")\n",
    "        self.set_init_datas() # classの変数 X_train, y_train, X_test, y_testに 実際の値を入力\n",
    "        \n",
    "        self.data_save = True\n",
    "        \n",
    "        \n",
    "    def load_csv_datas(self):\n",
    "        if self.data_save == False:\n",
    "            self.set_init_words()\n",
    "            self.set_init_datas()\n",
    "        \n",
    "    def load_data(self):\n",
    "        TN.set_init_words()\n",
    "        TN.set_init_datas()\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test, [ item for item, idx in self.word_to_index.items() ]\n",
    "        \n",
    "        \n",
    "    # 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "    # 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "    def get_encoded_sentence(self, sentence):\n",
    "        return [self.word_to_index[word] if word in self.word_to_index else self.word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "    # 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "    def get_encoded_sentences(self, sentences):\n",
    "        return [self.get_encoded_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    # 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentence(self, encoded_sentence):\n",
    "        return ' '.join(self.index_to_word[index] if index in self.index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "    # 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentences(self, encoded_sentences):\n",
    "        return [self.get_decoded_sentence(encoded_sentence) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "TN = ThubanNaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ratings_train.txt'と'ratings_test.txt'だけで   \n",
    "X_train, y_train, X_test, y_test, words_to_indexの値を得るためのclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スタート\n",
      "データセーブ、458.35976362228394秒でロード完了\n"
     ]
    }
   ],
   "source": [
    "print(\"スタート\")\n",
    "start = time.time()\n",
    "TN.save_datas_to_csv(num_sentences=999999)\n",
    "print(\"データセーブ、{}秒でロード完了\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを初めて扱っているならTN.save_datas_to_csv()関数を   \n",
    "そうじゃなければなければ   \n",
    "次の段階へ\n",
    "   \n",
    "### TN.save_datas_to_csv() この関数が結構時間がかかるため"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 145233, 테스트 개수: 48776\n",
      "単語数: 42412\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, words_to_index = TN.load_data()\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(X_train), len(X_test)))\n",
    "print(\"単語数: {}\".format(len(words_to_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_data(num_words=n)のnum_wordsで単語を何個持ってくるのかを決めます   \n",
    "存在する単語数より多い場合は存在する単語すべてを持ってきます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配列の長さをお互い同じくする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  10.838930152724874\n",
      "문장길이 최대 :  71\n",
      "문장길이 표준편차 :  9.036148727026863\n",
      "pad_sequences maxlen :  28\n",
      "전체 문장의 0.932889711302053%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配列の長さをお互い同じくする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print(\"clear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配列の長さをお互い同じくしなければ   \n",
    "モデルの訓練ができなくなります   \n",
    "   \n",
    "そしてpad_sequencesの関数の中でpadding='pre'はpreがいいらしい   \n",
    "X_train、X_testの配列一つ一つには文章があるんですが個々長さが違うので後ろのには空白が出ます   \n",
    "モデルの訓練の時空白が最後に計算して悪い影響が出るそうです   \n",
    "なので列の前の方に空白を置いて後ろは実際の文字を置くpreがいいらしいです"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル作成または訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 4)           169648    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 170,145\n",
      "Trainable params: 170,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words_to_index)    # 어휘 사전의 크기입니다\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드 출처 https://wikidocs.net/44249\n",
    "#검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 학습을 조기 종료(Early Stopping)합니다. \n",
    "#또한, ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장합니다.\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "#에포크는 총 15번을 수행하겠습니다. 또한 훈련 데이터 중 20%를 검증 데이터로 사용하면서 정확도를 확인합니다.\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
