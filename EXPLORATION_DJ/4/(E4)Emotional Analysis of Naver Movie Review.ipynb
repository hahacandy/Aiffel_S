{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準備完了\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "print(\"準備完了\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データを取るためにclass生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성\n"
     ]
    }
   ],
   "source": [
    "class ThubanNaver:\n",
    "\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    index_to_word, word_to_index = {}, {}\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"생성\")\n",
    "        \n",
    "    def set_init_words(self):\n",
    "        # set word dictionary\n",
    "        index_to_word = pd.read_csv('words.csv')\n",
    "        self.index_to_word = {idx:word for idx, word in index_to_word['word'].items()}\n",
    "        self.word_to_index = {word:idx for idx, word in self.index_to_word.items()}\n",
    "        \n",
    "    def set_init_datas(self):\n",
    "        read_train = pd.read_csv('train.csv').dropna()\n",
    "        read_test = pd.read_csv('test.csv').dropna()\n",
    "        \n",
    "        self.X_train = [self.get_encoded_sentence(text) for idx, text in read_train['text'].items()]\n",
    "        self.X_test = [self.get_encoded_sentence(text) for idx, text in read_test['text'].items()]\n",
    "        \n",
    "        self.y_train = [label for idx, label in read_train['label'].items()]\n",
    "        self.y_test = [label for idx, label in read_test['label'].items()]\n",
    "        \n",
    "    def save_datas_to_csv(self):\n",
    "        \n",
    "        # 데이터를 읽어봅시다. \n",
    "        train_txt = pd.read_table('ratings_train.txt')\n",
    "        test_txt = pd.read_table('ratings_test.txt')\n",
    "        # 重複のデータを削除, そしてNANを削除\n",
    "        train_txt = train_txt.drop_duplicates('document').dropna()\n",
    "        test_txt = test_txt.drop_duplicates('document').dropna()\n",
    "        \n",
    "        \n",
    "        okt = Okt()\n",
    "        \n",
    "        stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "        \n",
    "        #セーブされる配列\n",
    "        train_texts = []\n",
    "        test_texts = []\n",
    "        clean_words = []\n",
    "        \n",
    "\n",
    "        # 単語辞書をcsvファイルにセーブために韓国語だけまたはtoken化する\n",
    "        for i in range(0,2):\n",
    "            if i == 0:\n",
    "                read_x_datas = train_txt['document']\n",
    "                save_x_datas = train_texts\n",
    "            else:\n",
    "                read_x_datas = test_txt['document']\n",
    "                save_x_datas = test_texts\n",
    "                \n",
    "            for idx, item in enumerate(read_x_datas.items()):\n",
    "                item_text = re.compile('[가-힣]+').findall(item[1])\n",
    "                results = okt.morphs(' '.join(item_text))\n",
    "                stopwords_result = []\n",
    "                for result in results:\n",
    "                    if result not in stopwords: #単語たちのstopwordsを削除\n",
    "                        stopwords_result.append(result)\n",
    "                \n",
    "                # stopwordsしても文字がある場合だけ\n",
    "                if(len(stopwords_result) > 0):\n",
    "                    save_x_datas.append(' '.join(stopwords_result)) # trainまたはtestデータ\n",
    "                    clean_words.extend(stopwords_result) #辞書のcsvのため       \n",
    "                # print(\"{} {}\".format(results, idx))\n",
    "                \n",
    "                # 全部forしたら時間がかかるので（テスト用）\n",
    "                #if idx == 99:\n",
    "                #    break\n",
    "\n",
    "        #単語たちの重複を削除\n",
    "        clean_words = set(clean_words)\n",
    "        clean_words = list(clean_words)\n",
    "        \n",
    "        #\n",
    "        clean_words.insert(0, \"<UNUSED>\")\n",
    "        clean_words.insert(0, \"<UNK>\")\n",
    "        clean_words.insert(0, \"<BOS>\")\n",
    "        clean_words.insert(0, \"<PAD>\")\n",
    "        \n",
    "        \n",
    "        # words.csvセーブ\n",
    "        pd.DataFrame(clean_words, columns=['word']).to_csv('words.csv', encoding=\"utf-8\")\n",
    "        self.set_init_words() #下のtexts, labelsをセーブするため (classの変数 index_to_word, word_to_indexに 実際の値を入力)\n",
    "        \n",
    "        \n",
    "        # trainとtestをcsvでセーブする\n",
    "        train_labels = [result for result in train_txt['label']][:len(train_texts)]\n",
    "        test_labels = [result for result in test_txt['label']][:len(test_texts)]\n",
    "\n",
    "        train_df = pd.DataFrame({'text':train_texts, 'label':train_labels})\n",
    "        test_df = pd.DataFrame({'text':test_texts, 'label':test_labels})\n",
    "        \n",
    "        pd.DataFrame(train_df).to_csv('train.csv', encoding=\"utf-8\")\n",
    "        pd.DataFrame(test_df).to_csv('test.csv', encoding=\"utf-8\")\n",
    "        self.set_init_datas() # classの変数 X_train, y_train, X_test, y_testに 実際の値を入力\n",
    "        \n",
    "        \n",
    "    def load_csv_datas(self):\n",
    "        self.set_init_words()\n",
    "        self.set_init_datas()\n",
    "        \n",
    "    def load_data(self, num_words):\n",
    "        if len(self.word_to_index.items()) < num_words: #num_words数がword_to_index配列の長さより長い場合はword_to_indexの長さに従う\n",
    "            num_words = len(self.word_to_index.items())\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test, [ item for item, idx in self.word_to_index.items() if idx < int(num_words) ]\n",
    "        \n",
    "        \n",
    "    # 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "    # 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "    def get_encoded_sentence(self, sentence):\n",
    "        return [self.word_to_index['<BOS>']]+[self.word_to_index[word] if word in self.word_to_index else self.word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "    # 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "    def get_encoded_sentences(self, sentences):\n",
    "        return [self.get_encoded_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    # 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentence(self, encoded_sentence):\n",
    "        return ' '.join(self.index_to_word[index] if index in self.index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "    # 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentences(self, encoded_sentences):\n",
    "        return [self.get_decoded_sentence(encoded_sentence, self.index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "TN = ThubanNaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ratings_train.txt'と'ratings_test.txt'だけで   \n",
    "X_train, y_train, X_test, y_test, words_to_indexの値を得るためのclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スタート\n"
     ]
    }
   ],
   "source": [
    "print(\"スタート\")\n",
    "TN.save_datas_to_csv()\n",
    "print(\"データセーブ、ロード完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すでにTN.save_datas_to_csv()をして words.csv, x_test.csv, x_train.csv ファイルが居る場合は下の関数を利用してデータを読み込みする\n",
    "TN.set_init_words()\n",
    "TN.set_init_datas()\n",
    "print(\"データロード完了\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを初めて扱っているならTN.save_datas_to_csv()関数を   \n",
    "そうじゃなければなければ   \n",
    "TN.set_init_words()   \n",
    "TN.set_init_datas()   \n",
    "を使ってください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, words_to_index = TN.load_data(num_words=100000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(X_train), len(X_test)))\n",
    "print(\"単語数: {}\".format(len(words_to_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_data(num_words=100000)のnum_wordsで単語を何個持ってくるのかを決めます   \n",
    "存在する単語数より多い場合は存在する単語すべてを持ってきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print()\n",
    "print(TN.get_decoded_sentence(X_train[112310]))\n",
    "print(y_train[112310])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN.get_encoded_sentence(\"먼가 많이 이상해 그렇게 생각하지 않아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "print(okt.morphs(\"안녕 하세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform \n",
    "print(platform.architecture())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
