{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準備完了\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "print(\"準備完了\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データを取るためにclass生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성\n"
     ]
    }
   ],
   "source": [
    "class ThubanNaver:\n",
    "\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    index_to_word, word_to_index = {}, {}\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"생성\")\n",
    "        \n",
    "    def set_init_words(self):\n",
    "        # set word dictionary\n",
    "        index_to_word = pd.read_csv('words.csv')\n",
    "        self.index_to_word = {idx:word for idx, word in index_to_word['word'].items()}\n",
    "        self.word_to_index = {word:idx for idx, word in self.index_to_word.items()}\n",
    "        \n",
    "    def set_init_datas(self):\n",
    "        read_train = pd.read_csv('train.csv').dropna()\n",
    "        read_test = pd.read_csv('test.csv').dropna()\n",
    "        \n",
    "        self.X_train = [self.get_encoded_sentence(text) for idx, text in read_train['text'].items()]\n",
    "        self.X_test = [self.get_encoded_sentence(text) for idx, text in read_test['text'].items()]\n",
    "        \n",
    "        self.y_train = np.array([label for idx, label in read_train['label'].items()])\n",
    "        self.y_test = np.array([label for idx, label in read_test['label'].items()])\n",
    "        \n",
    "    def save_datas_to_csv(self):\n",
    "        \n",
    "        # 데이터를 읽어봅시다. \n",
    "        train_txt = pd.read_table('ratings_train.txt')\n",
    "        test_txt = pd.read_table('ratings_test.txt')\n",
    "        # 重複のデータを削除, そしてNANを削除\n",
    "        train_txt = train_txt.drop_duplicates('document').dropna()\n",
    "        test_txt = test_txt.drop_duplicates('document').dropna()\n",
    "        \n",
    "        \n",
    "        okt = Okt()\n",
    "        \n",
    "        stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "        \n",
    "        #セーブされる配列\n",
    "        train_texts = []\n",
    "        test_texts = []\n",
    "        clean_words = []\n",
    "        \n",
    "\n",
    "        # 単語辞書をcsvファイルにセーブために韓国語だけまたはtoken化する\n",
    "        for i in range(0,2):\n",
    "            if i == 0:\n",
    "                read_x_datas = train_txt['document']\n",
    "                save_x_datas = train_texts\n",
    "            else:\n",
    "                read_x_datas = test_txt['document']\n",
    "                save_x_datas = test_texts\n",
    "                \n",
    "            for idx, item in enumerate(read_x_datas.items()):\n",
    "                item_text = re.compile('[가-힣]+').findall(item[1])\n",
    "                results = okt.morphs(' '.join(item_text))\n",
    "                stopwords_result = []\n",
    "                for result in results:\n",
    "                    if result not in stopwords: #単語たちのstopwordsを削除\n",
    "                        stopwords_result.append(result)\n",
    "                \n",
    "                # stopwordsしても文字がある場合だけ\n",
    "                if(len(stopwords_result) > 0):\n",
    "                    save_x_datas.append(' '.join(stopwords_result)) # trainまたはtestデータ\n",
    "                    clean_words.extend(stopwords_result) #辞書のcsvのため       \n",
    "                # print(\"{} {}\".format(results, idx))\n",
    "                \n",
    "                # 全部forしたら時間がかかるので（テスト用）\n",
    "                #if idx == 99:\n",
    "                #    break\n",
    "\n",
    "        #単語たちの重複を削除\n",
    "        clean_words = set(clean_words)\n",
    "        clean_words = list(clean_words)\n",
    "        \n",
    "        #\n",
    "        clean_words.insert(0, \"<UNUSED>\")\n",
    "        clean_words.insert(0, \"<UNK>\")\n",
    "        clean_words.insert(0, \"<BOS>\")\n",
    "        clean_words.insert(0, \"<PAD>\")\n",
    "        \n",
    "        \n",
    "        # words.csvセーブ\n",
    "        pd.DataFrame(clean_words, columns=['word']).to_csv('words.csv', encoding=\"utf-8\")\n",
    "        self.set_init_words() #下のtexts, labelsをセーブするため (classの変数 index_to_word, word_to_indexに 実際の値を入力)\n",
    "        \n",
    "        \n",
    "        # trainとtestをcsvでセーブする\n",
    "        train_labels = [result for result in train_txt['label']][:len(train_texts)]\n",
    "        test_labels = [result for result in test_txt['label']][:len(test_texts)]\n",
    "\n",
    "        train_df = pd.DataFrame({'text':train_texts, 'label':train_labels})\n",
    "        test_df = pd.DataFrame({'text':test_texts, 'label':test_labels})\n",
    "        \n",
    "        pd.DataFrame(train_df).to_csv('train.csv', encoding=\"utf-8\")\n",
    "        pd.DataFrame(test_df).to_csv('test.csv', encoding=\"utf-8\")\n",
    "        self.set_init_datas() # classの変数 X_train, y_train, X_test, y_testに 実際の値を入力\n",
    "        \n",
    "        \n",
    "    def load_csv_datas(self):\n",
    "        self.set_init_words()\n",
    "        self.set_init_datas()\n",
    "        \n",
    "    def load_data(self, num_words):\n",
    "        if len(self.word_to_index.items()) < num_words: #num_words数がword_to_index配列の長さより長い場合はword_to_indexの長さに従う\n",
    "            num_words = len(self.word_to_index.items())\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test, [ item for item, idx in self.word_to_index.items() if idx < int(num_words) ]\n",
    "        \n",
    "        \n",
    "    # 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "    # 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "    def get_encoded_sentence(self, sentence):\n",
    "        return [self.word_to_index['<BOS>']]+[self.word_to_index[word] if word in self.word_to_index else self.word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "    # 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "    def get_encoded_sentences(self, sentences):\n",
    "        return [self.get_encoded_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    # 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentence(self, encoded_sentence):\n",
    "        return ' '.join(self.index_to_word[index] if index in self.index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "    # 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentences(self, encoded_sentences):\n",
    "        return [self.get_decoded_sentence(encoded_sentence, self.index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "TN = ThubanNaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ratings_train.txt'と'ratings_test.txt'だけで   \n",
    "X_train, y_train, X_test, y_test, words_to_indexの値を得るためのclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"スタート\")\n",
    "TN.save_datas_to_csv()\n",
    "print(\"データセーブ、ロード完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データロード完了\n"
     ]
    }
   ],
   "source": [
    "# すでにTN.save_datas_to_csv()をして words.csv, x_test.csv, x_train.csv ファイルが居る場合は下の関数を利用してデータを読み込みする\n",
    "TN.set_init_words()\n",
    "TN.set_init_datas()\n",
    "print(\"データロード完了\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを初めて扱っているならTN.save_datas_to_csv()関数を   \n",
    "そうじゃなければなければ   \n",
    "TN.set_init_words()   \n",
    "TN.set_init_datas()   \n",
    "を使ってください   \n",
    "   \n",
    "### こうやって二つのコードに割った理由は TN.save_datas_to_csv() この関数が結構時間がかかるため"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수: 145237, 테스트 개수: 48773\n",
      "単語数: 113661\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, words_to_index = TN.load_data(num_words=113661)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(X_train), len(X_test)))\n",
    "print(\"単語数: {}\".format(len(words_to_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_data(num_words=n)のnum_wordsで単語を何個持ってくるのかを決めます   \n",
    "存在する単語数より多い場合は存在する単語すべてを持ってきます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_to_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a94e1a0de3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_to_word' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = 113661    # 어휘 사전의 크기입니다\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配列の長さをお互い同じくする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配列の長さをお互い同じくする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=TN.word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=TN.word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(\"clear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配列の長さをお互い同じくしなければ   \n",
    "モデルの訓練ができなくなります   \n",
    "   \n",
    "そしてpad_sequencesの関数の中でpadding='pre'はpreがいいらしい   \n",
    "X_train、X_testの配列一つ一つには文章があるんですが個々長さが違うので後ろのには空白が出ます   \n",
    "モデルの訓練の時空白が最後に計算して悪い影響が出るそうです   \n",
    "なので列の前の方に空白を置いて後ろは実際の文字を置くpreがいいらしいです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの正確性を上げるためTrainingとValidationで割る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set 50000건 분리\n",
    "x_val = X_train[:50000]   \n",
    "y_val = y_train[:50000]\n",
    "\n",
    "# validation set을 제외한 나머지\n",
    "partial_x_train = X_train[50000:]  \n",
    "partial_y_train = y_train[50000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingデータ95237列   \n",
    "Validationデータ50000列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
