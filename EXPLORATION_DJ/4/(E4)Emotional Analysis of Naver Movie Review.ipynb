{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準備完了\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "print(\"準備完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThubanNaver:\n",
    "    train_datas, test_datas = [], []\n",
    "    index_to_word, word_to_index = {}, {}\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 데이터를 읽어봅시다. \n",
    "        self.train_data = pd.read_table('ratings_train.txt')\n",
    "        self.test_data = pd.read_table('ratings_test.txt')\n",
    "        \n",
    "        # 重複のデータを削除, そしてNANを削除\n",
    "        self.train_data = self.train_data.drop_duplicates('document').dropna()\n",
    "        self.test_data = self.test_data.drop_duplicates('document').dropna()\n",
    "    \n",
    "\n",
    "    def set_init_words(self):\n",
    "        # set word dictionary\n",
    "        index_to_word = pd.read_csv('words.csv')\n",
    "        self.index_to_word = {idx:word for idx, word in index_to_word['word'].items()}\n",
    "        self.word_to_index = {word:idx for idx, word in self.index_to_word.items()}\n",
    "        \n",
    "    def set_init_datas(self):\n",
    "        read_x_train = pd.read_csv('x_train.csv')\n",
    "        read_x_test = pd.read_csv('x_test.csv')\n",
    "        \n",
    "        self.train_datas = [self.get_encoded_sentences(result) for result in read_x_train]\n",
    "        self.test_datas = [self.get_encoded_sentences(result) for result in read_x_test]\n",
    "        \n",
    "    def save_datas_to_csv(self):\n",
    "        \n",
    "        okt = Okt()\n",
    "        \n",
    "        stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "        \n",
    "        #セーブされる配列\n",
    "        train_datas = []\n",
    "        test_datas = []\n",
    "        clean_words = []\n",
    "        \n",
    "\n",
    "        # 単語辞書をcsvファイルにセーブために韓国語だけまたはtoken化する\n",
    "        for i in range(0,2):\n",
    "            if i == 0:\n",
    "                read_x_datas = self.train_data['document']\n",
    "                save_x_datas = train_datas\n",
    "            else:\n",
    "                ml_x_datas = self.test_data['document']\n",
    "                save_x_datas = test_datas\n",
    "                \n",
    "            for idx, item in enumerate(read_x_datas.items()):\n",
    "                item_text = re.compile('[가-힣]+').findall(item[1])\n",
    "                results = okt.morphs(' '.join(item_text))\n",
    "                stopwords_result = []\n",
    "                for result in results:\n",
    "                    if result not in stopwords: #単語たちのstopwordsを削除\n",
    "                        stopwords_result.append(result)\n",
    "                \n",
    "                \n",
    "                save_x_datas.append(' '.join(stopwords_result)) # trainまたはtestデータ\n",
    "                clean_words.extend(stopwords_result) #辞書のcsvのため       \n",
    "                # print(\"{} {}\".format(results, idx))\n",
    "                \n",
    "                if idx > 100:\n",
    "                    break\n",
    "        \n",
    "\n",
    "        #単語たちの重複を削除\n",
    "        clean_words = set(clean_words)\n",
    "        clean_words = list(clean_words)\n",
    "        \n",
    "        #\n",
    "        clean_words.insert(0, \"<UNUSED>\")\n",
    "        clean_words.insert(0, \"<UNK>\")\n",
    "        clean_words.insert(0, \"<BOS>\")\n",
    "        clean_words.insert(0, \"<PAD>\")\n",
    "        \n",
    "        \n",
    "        pd.DataFrame(clean_words, columns=['word']).to_csv('words.csv', encoding=\"utf-8\")\n",
    "        self.set_init_words() #下のtrain_datas,test_datasをセーブするため (classの変数 index_to_word, word_to_indexに 実際の値を入力)\n",
    "        \n",
    "        pd.DataFrame(train_datas, columns=['text']).to_csv('x_train.csv', encoding=\"utf-8\")\n",
    "        pd.DataFrame(test_datas, columns=['text']).to_csv('x_test.csv', encoding=\"utf-8\")\n",
    "        self.set_init_datas() # classの変数 train_datas, test_datasに 実際の値を入力\n",
    "        \n",
    "    def load_csv_datas(self):\n",
    "        self.set_init_words()\n",
    "        self.set_init_datas()\n",
    "        \n",
    "        \n",
    "    # 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "    # 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "    def get_encoded_sentence(self, sentence):\n",
    "        return [self.word_to_index['<BOS>']]+[self.word_to_index[word] if word in self.word_to_index else self.word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "    # 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "    def get_encoded_sentences(self, sentences):\n",
    "        return [self.get_encoded_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    \n",
    "\n",
    "TN = ThubanNaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "TN.save_datas_to_csv()\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN.set_init_datas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         1\n",
       "2         0\n",
       "3         0\n",
       "4         1\n",
       "         ..\n",
       "149995    0\n",
       "149996    1\n",
       "149997    0\n",
       "149998    1\n",
       "149999    0\n",
       "Name: label, Length: 146182, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN.train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN.get_encoded_sentence(\"먼가 많이 이상해 그렇게 생각하지 않아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "print(okt.morphs(\"안녕 하세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform \n",
    "print(platform.architecture())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
