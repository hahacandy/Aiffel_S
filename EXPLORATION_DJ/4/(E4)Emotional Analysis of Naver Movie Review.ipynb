{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準備完了\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "print(\"準備完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성\n"
     ]
    }
   ],
   "source": [
    "class ThubanNaver:\n",
    "\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    index_to_word, word_to_index = {}, {}\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"생성\")\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def set_init_words(self):\n",
    "        # set word dictionary\n",
    "        index_to_word = pd.read_csv('words.csv')\n",
    "        self.index_to_word = {idx:word for idx, word in index_to_word['word'].items()}\n",
    "        self.word_to_index = {word:idx for idx, word in self.index_to_word.items()}\n",
    "        \n",
    "    def set_init_datas(self):\n",
    "        read_train = pd.read_csv('train.csv')\n",
    "        read_test = pd.read_csv('test.csv')\n",
    "        \n",
    "        self.X_train = [self.get_encoded_sentence(text) for idx, text in read_train['text'].items()]\n",
    "        self.X_test = [self.get_encoded_sentence(text) for idx, text in read_test['text'].items()]\n",
    "        \n",
    "        self.y_train = [label for idx, label in read_train['label'].items()]\n",
    "        self.y_test = [label for idx, label in read_test['label'].items()]\n",
    "        \n",
    "    def save_datas_to_csv(self):\n",
    "        \n",
    "        # 데이터를 읽어봅시다. \n",
    "        train_txt = pd.read_table('ratings_train.txt')\n",
    "        test_txt = pd.read_table('ratings_test.txt')\n",
    "        # 重複のデータを削除, そしてNANを削除\n",
    "        train_txt = train_txt.drop_duplicates('document').dropna()\n",
    "        test_txt = test_txt.drop_duplicates('document').dropna()\n",
    "        \n",
    "        \n",
    "        okt = Okt()\n",
    "        \n",
    "        stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "        \n",
    "        #セーブされる配列\n",
    "        train_texts = []\n",
    "        test_texts = []\n",
    "        clean_words = []\n",
    "        \n",
    "\n",
    "        # 単語辞書をcsvファイルにセーブために韓国語だけまたはtoken化する\n",
    "        for i in range(0,2):\n",
    "            if i == 0:\n",
    "                read_x_datas = train_txt['document']\n",
    "                save_x_datas = train_texts\n",
    "            else:\n",
    "                ml_x_datas = test_txt['document']\n",
    "                save_x_datas = test_texts\n",
    "                \n",
    "            for idx, item in enumerate(read_x_datas.items()):\n",
    "                item_text = re.compile('[가-힣]+').findall(item[1])\n",
    "                results = okt.morphs(' '.join(item_text))\n",
    "                stopwords_result = []\n",
    "                for result in results:\n",
    "                    if result not in stopwords: #単語たちのstopwordsを削除\n",
    "                        stopwords_result.append(result)\n",
    "                \n",
    "                \n",
    "                save_x_datas.append(' '.join(stopwords_result)) # trainまたはtestデータ\n",
    "                clean_words.extend(stopwords_result) #辞書のcsvのため       \n",
    "                # print(\"{} {}\".format(results, idx))\n",
    "                \n",
    "                if idx == 99:\n",
    "                    break\n",
    "\n",
    "        #単語たちの重複を削除\n",
    "        clean_words = set(clean_words)\n",
    "        clean_words = list(clean_words)\n",
    "        \n",
    "        #\n",
    "        clean_words.insert(0, \"<UNUSED>\")\n",
    "        clean_words.insert(0, \"<UNK>\")\n",
    "        clean_words.insert(0, \"<BOS>\")\n",
    "        clean_words.insert(0, \"<PAD>\")\n",
    "        \n",
    "        \n",
    "        # words.csvセーブ\n",
    "        pd.DataFrame(clean_words, columns=['word']).to_csv('words.csv', encoding=\"utf-8\")\n",
    "        self.set_init_words() #下のtexts, labelsをセーブするため (classの変数 index_to_word, word_to_indexに 実際の値を入力)\n",
    "        \n",
    "        \n",
    "        # trainとtestをcsvでセーブする\n",
    "        train_labels = [result for result in train_txt['label']][:len(train_texts)]\n",
    "        test_labels = [result for result in test_txt['label']][:len(test_texts)]\n",
    "\n",
    "        train_df = pd.DataFrame({'text':train_texts, 'label':train_labels})\n",
    "        test_df = pd.DataFrame({'text':train_texts, 'label':train_labels})\n",
    "        \n",
    "        pd.DataFrame(train_df).to_csv('train.csv', encoding=\"utf-8\")\n",
    "        pd.DataFrame(test_df).to_csv('test.csv', encoding=\"utf-8\")\n",
    "        self.set_init_datas() # classの変数 X_train, y_train, X_test, y_testに 実際の値を入力\n",
    "        \n",
    "        \n",
    "    def load_csv_datas(self):\n",
    "        self.set_init_words()\n",
    "        self.set_init_datas()\n",
    "        \n",
    "        \n",
    "    # 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "    # 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "    def get_encoded_sentence(self, sentence):\n",
    "        return [self.word_to_index['<BOS>']]+[self.word_to_index[word] if word in self.word_to_index else self.word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "    # 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "    def get_encoded_sentences(self, sentences):\n",
    "        return [self.get_encoded_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    # 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentence(self, encoded_sentence):\n",
    "        return ' '.join(self.index_to_word[index] if index in self.index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "    # 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "    def get_decoded_sentences(self, encoded_sentences):\n",
    "        return [self.get_decoded_sentence(encoded_sentence, self.index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "TN = ThubanNaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データセーブ完了\n"
     ]
    }
   ],
   "source": [
    "TN.save_datas_to_csv()\n",
    "print(\"データセーブ完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すでにTN.save_datas_to_csv()をして words.csv, x_test.csv, x_train.csv ファイルが居る場合は下の関数を利用してデータを読み込みする\n",
    "TN.set_init_words()\n",
    "TN.set_init_datas()\n",
    "print(\"データロード完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙 진짜 짜증나네요 목소리\n",
      "0\n",
      "\n",
      "흠 포스터 보고 초딩 영화 줄 오버 연기 조차 가볍지 않구나\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(TN.get_decoded_sentence(TN.X_train[0]))\n",
    "print(TN.y_train[0])\n",
    "print()\n",
    "print(TN.get_decoded_sentence(TN.X_train[1]))\n",
    "print(TN.y_train[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN.get_encoded_sentence(\"먼가 많이 이상해 그렇게 생각하지 않아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "print(okt.morphs(\"안녕 하세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform \n",
    "print(platform.architecture())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
