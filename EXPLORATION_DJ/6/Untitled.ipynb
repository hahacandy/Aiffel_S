{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "damaged-dominican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import os\n",
    "\n",
    "# 파일을 읽기모드로 열어 봅니다.\n",
    "file_path = 'lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()   # 텍스트를 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "\n",
    "print(raw_corpus[:9])    # 앞에서부터 10라인만 화면에 출력해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coupled-concentration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "temporal-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modified-stamp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "promotional-contents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fa853a60910>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "soviet-provision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "recent-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "appointed-adolescent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "industrial-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24015\n"
     ]
    }
   ],
   "source": [
    "print(len(src_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "double-confidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "serious-wheel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "print(\"clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "monetary-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-1.6937460e-05, -3.8940405e-05,  3.9866513e-06, ...,\n",
       "         -2.6324808e-04,  9.2062539e-05,  1.2825799e-04],\n",
       "        [ 5.1323150e-04, -2.0497320e-04, -2.7392613e-04, ...,\n",
       "         -1.6850635e-04,  1.6346347e-04,  4.5432433e-04],\n",
       "        [ 8.3349313e-04, -1.1804059e-04, -1.0198162e-04, ...,\n",
       "          1.7969830e-04,  9.2099370e-05,  1.5625660e-04],\n",
       "        ...,\n",
       "        [-2.4435851e-03, -2.1827295e-03,  2.7194815e-03, ...,\n",
       "         -1.3682047e-03,  1.1236659e-03, -1.9333201e-03],\n",
       "        [-2.5969448e-03, -2.2801044e-03,  3.2283538e-03, ...,\n",
       "         -1.5753768e-03,  1.2311439e-03, -2.3447296e-03],\n",
       "        [-2.6953085e-03, -2.3588336e-03,  3.6876954e-03, ...,\n",
       "         -1.7033081e-03,  1.3150672e-03, -2.7447836e-03]],\n",
       "\n",
       "       [[-1.6937460e-05, -3.8940405e-05,  3.9866513e-06, ...,\n",
       "         -2.6324808e-04,  9.2062539e-05,  1.2825799e-04],\n",
       "        [-4.3233251e-04,  9.5199684e-06,  3.4869328e-04, ...,\n",
       "         -6.6925125e-04,  4.9667706e-05, -1.0743536e-04],\n",
       "        [-1.8674225e-04,  7.6097400e-05,  5.4975320e-04, ...,\n",
       "         -8.1473519e-04, -1.5727484e-04, -3.7668759e-04],\n",
       "        ...,\n",
       "        [-2.2073635e-03, -1.6243070e-03,  2.9497207e-03, ...,\n",
       "         -9.8157662e-04,  1.0191834e-03, -2.5680983e-03],\n",
       "        [-2.4477867e-03, -1.8384922e-03,  3.4013425e-03, ...,\n",
       "         -1.2596633e-03,  1.2134199e-03, -2.8890716e-03],\n",
       "        [-2.6176546e-03, -2.0150603e-03,  3.8153103e-03, ...,\n",
       "         -1.4597405e-03,  1.3684974e-03, -3.2033799e-03]],\n",
       "\n",
       "       [[-1.6937460e-05, -3.8940405e-05,  3.9866513e-06, ...,\n",
       "         -2.6324808e-04,  9.2062539e-05,  1.2825799e-04],\n",
       "        [ 1.5905821e-04, -3.1148276e-04, -1.6729439e-04, ...,\n",
       "         -3.2958997e-04,  3.3117458e-04,  2.8198972e-04],\n",
       "        [ 4.8354952e-04, -1.8868630e-04,  2.2192403e-04, ...,\n",
       "         -1.8482056e-04,  2.5180372e-04,  3.2175241e-05],\n",
       "        ...,\n",
       "        [-2.4019401e-03, -1.4125652e-03,  3.0895546e-03, ...,\n",
       "         -2.3197487e-03,  7.8716397e-04, -1.5689596e-03],\n",
       "        [-2.4534208e-03, -1.6777444e-03,  3.4705228e-03, ...,\n",
       "         -2.5759465e-03,  9.7005430e-04, -2.0725005e-03],\n",
       "        [-2.4807965e-03, -1.8900817e-03,  3.8298303e-03, ...,\n",
       "         -2.7096702e-03,  1.1196001e-03, -2.5367455e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.6937460e-05, -3.8940405e-05,  3.9866513e-06, ...,\n",
       "         -2.6324808e-04,  9.2062539e-05,  1.2825799e-04],\n",
       "        [-3.1234213e-04, -4.4958408e-05, -3.5488995e-04, ...,\n",
       "         -7.7718013e-04,  2.2652796e-06,  1.9535846e-04],\n",
       "        [-4.9014814e-04, -1.3559089e-05, -8.2830602e-04, ...,\n",
       "         -1.0274297e-03,  9.1340575e-05,  6.8971107e-04],\n",
       "        ...,\n",
       "        [-2.0964146e-03, -2.5667276e-03,  5.0813551e-03, ...,\n",
       "         -1.7079504e-03,  1.7846096e-03, -3.9325771e-03],\n",
       "        [-2.1644209e-03, -2.6592002e-03,  5.3104246e-03, ...,\n",
       "         -1.6851812e-03,  1.8191040e-03, -4.2107510e-03],\n",
       "        [-2.2193261e-03, -2.7452917e-03,  5.5006123e-03, ...,\n",
       "         -1.6379235e-03,  1.8383658e-03, -4.4562244e-03]],\n",
       "\n",
       "       [[-1.6937460e-05, -3.8940405e-05,  3.9866513e-06, ...,\n",
       "         -2.6324808e-04,  9.2062539e-05,  1.2825799e-04],\n",
       "        [-7.7279772e-05,  2.5188597e-04, -9.3192095e-05, ...,\n",
       "         -5.0206028e-04,  1.8060760e-04,  2.1254714e-04],\n",
       "        [-3.0019469e-04,  3.7189902e-04, -3.7523091e-04, ...,\n",
       "         -5.5950077e-04,  2.5879877e-04,  3.5503585e-05],\n",
       "        ...,\n",
       "        [-1.1456381e-03, -1.1821666e-03,  1.8740098e-03, ...,\n",
       "         -2.7431899e-03,  6.3143950e-04, -1.8366210e-03],\n",
       "        [-1.2976025e-03, -1.3919002e-03,  2.4704351e-03, ...,\n",
       "         -2.7856703e-03,  8.0046884e-04, -2.2760683e-03],\n",
       "        [-1.4297847e-03, -1.5826367e-03,  3.0260119e-03, ...,\n",
       "         -2.7631603e-03,  9.4828318e-04, -2.7019118e-03]],\n",
       "\n",
       "       [[-1.6937460e-05, -3.8940405e-05,  3.9866513e-06, ...,\n",
       "         -2.6324808e-04,  9.2062539e-05,  1.2825799e-04],\n",
       "        [-1.8444067e-04, -1.3795178e-04, -5.8778816e-05, ...,\n",
       "         -8.6460932e-04,  8.5026193e-05,  2.1282323e-04],\n",
       "        [-1.3003482e-04, -3.2278098e-04, -2.5338322e-04, ...,\n",
       "         -1.0774393e-03, -1.0776558e-04,  1.6131025e-04],\n",
       "        ...,\n",
       "        [-1.9182255e-03, -1.7432019e-03,  3.9095352e-03, ...,\n",
       "         -2.1561440e-03,  1.1972855e-03, -4.1499040e-03],\n",
       "        [-1.9441246e-03, -1.9153814e-03,  4.2929552e-03, ...,\n",
       "         -2.1639697e-03,  1.2944862e-03, -4.3841568e-03],\n",
       "        [-1.9725061e-03, -2.0759725e-03,  4.6284720e-03, ...,\n",
       "         -2.1278092e-03,  1.3737943e-03, -4.5984592e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stupid-heading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "metric-indian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 12s 130ms/step - loss: 3.5772\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 12s 131ms/step - loss: 2.8263\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 2.7554\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.6713\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 13s 135ms/step - loss: 2.6028\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 12s 132ms/step - loss: 2.5440\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 13s 135ms/step - loss: 2.4892\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 2.4427\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 2.4017\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 13s 135ms/step - loss: 2.3637\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 2.3268\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 2.2924\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.2599\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 2.2282\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.1957\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 2.1642\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 2.1333\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 2.1022\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.0711\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.0385\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 2.0065\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 1.9754\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 1.9448\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 1.9143\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 1.8840\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 1.8518\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 12s 133ms/step - loss: 1.8229\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 12s 133ms/step - loss: 1.7927\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 12s 133ms/step - loss: 1.7644\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 12s 133ms/step - loss: 1.7347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa853a60250>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "apart-tissue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다.\n",
    "\n",
    "\n",
    "print(\"clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "economic-chess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> why , then , i pray , what s the matter ? <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> why\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
