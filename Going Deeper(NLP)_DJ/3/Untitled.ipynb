{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joined-platinum",
   "metadata": {},
   "source": [
    "# 3-1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-ticket",
   "metadata": {},
   "source": [
    "$ mkdir -p ~/aiffel/topic_modelling/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-fence",
   "metadata": {},
   "source": [
    "# 3-2. 단어 빈도를 이용한 벡터화 (1) Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "patient-operation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:25:41.145008Z",
     "start_time": "2021-04-15T02:25:41.142281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-english",
   "metadata": {},
   "source": [
    "# 3-3. 단어 빈도를 이용한 벡터화 (2) Bag of Words 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "little-mother",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:26:47.331484Z",
     "start_time": "2021-04-15T02:26:46.168962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words : {'john': 1, 'likes': 3, 'to': 2, 'watch': 2, 'movies': 2, 'mary': 2, 'too': 1, 'also': 1, 'football': 1, 'games': 1}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "# keras Tokenizer 활용\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
    "\n",
    "def print_bow(sentence):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence) # 단어장 생성\n",
    "    bow = dict(tokenizer.word_counts) # 각 단어와 각 단어의 빈도를 bow에 저장\n",
    "    print(\"Bag of Words :\", bow) # bow 출력\n",
    "    print('단어장(Vocabulary)의 크기 :', len(tokenizer.word_counts)) # 중복을 제거한 단어들의 개수\n",
    "\n",
    "print_bow(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "superb-feeding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:29:29.773082Z",
     "start_time": "2021-04-15T02:29:29.769170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[1 1 1 1 3 2 2 2 1 2]]\n",
      "각 단어의 인덱스 : {'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn CountVectorizer 활용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
    "vector = CountVectorizer()\n",
    "print('Bag of Words : ',vector.fit_transform(sentence).toarray()) # 코퍼스로부터 각 단어의 빈도수를 기록한다.\n",
    "print('각 단어의 인덱스 :',vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
    "print('단어장(Vocabulary)의 크기 :', len(vector.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-collector",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:28:16.651872Z",
     "start_time": "2021-04-15T02:28:16.649234Z"
    }
   },
   "source": [
    "# 3-4. 단어 빈도를 이용한 벡터화 (3) DTM과 코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "musical-contact",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:38:50.981036Z",
     "start_time": "2021-04-15T02:38:50.977780Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "doc1 = np.array([0,1,1,1]) # 문서1 벡터\n",
    "doc2 = np.array([1,0,1,1]) # 문서2 벡터\n",
    "doc3 = np.array([2,0,2,2]) # 문서3 벡터\n",
    "\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "partial-omega",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:40:03.956612Z",
     "start_time": "2021-04-15T02:40:03.953493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "0.6666666666666667\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(doc1, doc2)) #문서1과 문서2의 코사인 유사도\n",
    "print(cos_sim(doc1, doc3)) #문서1과 문서3의 코사인 유사도\n",
    "print(cos_sim(doc2, doc3)) #문서2과 문서3의 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-resistance",
   "metadata": {},
   "source": [
    "# 3-5. 단어 빈도를 이용한 벡터화 (4) DTM의 구현과 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nuclear-training",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:40:55.648841Z",
     "start_time": "2021-04-15T02:40:55.634217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 1 0 1]\n",
      " [0 0 0 0 1 1 1 0 1 0]\n",
      " [1 1 1 0 1 1 0 1 0 1]]\n",
      "{'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn CountVectorizer 활용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'John likes to watch movies',\n",
    "    'Mary likes movies too',\n",
    "    'Mary also likes to watch football games',    \n",
    "]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도수를 기록.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-capitol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:43:05.946097Z",
     "start_time": "2021-04-15T02:43:05.944266Z"
    }
   },
   "source": [
    "# 3-6. 단어 빈도를 이용한 벡터화 (5) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "renewable-nelson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:19:57.326511Z",
     "start_time": "2021-04-15T04:19:57.319317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.954451150103322"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(12)*math.sqrt(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://youtu.be/Rd3OnBPDRbM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-technical",
   "metadata": {},
   "source": [
    "# 3-7. 단어 빈도를 이용한 벡터화 (6) TF-IDF 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "northern-fortune",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:38:22.798101Z",
     "start_time": "2021-04-15T04:38:22.795801Z"
    }
   },
   "outputs": [],
   "source": [
    "#TF-IDF 구현하기\n",
    "from math import log\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hungry-development",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:38:26.333696Z",
     "start_time": "2021-04-15T04:38:26.331768Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "  'John likes to watch movies and Mary likes movies too',\n",
    "  'James likes to watch TV',\n",
    "  'Mary also likes to watch football games',  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "recreational-reserve",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:38:29.909922Z",
     "start_time": "2021-04-15T04:38:29.906925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 13\n",
      "['James', 'John', 'Mary', 'TV', 'also', 'and', 'football', 'games', 'likes', 'movies', 'to', 'too', 'watch']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()\n",
    "print('단어장의 크기 :', len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "generous-bridges",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:40:22.830729Z",
     "start_time": "2021-04-15T04:40:22.827359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(docs) # 총 문서의 수\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "behind-vessel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:41:14.310093Z",
     "start_time": "2021-04-15T04:41:14.307222Z"
    }
   },
   "outputs": [],
   "source": [
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    " \n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc    \n",
    "    return log(N/(df + 1)) + 1\n",
    " \n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "known-machinery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:41:19.985799Z",
     "start_time": "2021-04-15T04:41:19.971223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>James</th>\n",
       "      <th>John</th>\n",
       "      <th>Mary</th>\n",
       "      <th>TV</th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>likes</th>\n",
       "      <th>movies</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   James  John  Mary  TV  also  and  football  games  likes  movies  to  too  \\\n",
       "0      0     1     1   0     0    1         0      0      2       2   2    1   \n",
       "1      1     0     0   1     0    0         0      0      1       0   1    0   \n",
       "2      0     0     1   0     1    0         1      1      1       0   1    0   \n",
       "\n",
       "   watch  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N): # 각 문서에 대해서 아래 명령을 수행\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        \n",
    "        result[-1].append(tf(t, d))\n",
    "        \n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "annual-storage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:41:31.826351Z",
     "start_time": "2021-04-15T04:41:31.819749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>James</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mary</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TV</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>football</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>games</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likes</th>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movies</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>too</th>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watch</th>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               IDF\n",
       "James     1.405465\n",
       "John      1.405465\n",
       "Mary      1.000000\n",
       "TV        1.405465\n",
       "also      1.405465\n",
       "and       1.405465\n",
       "football  1.405465\n",
       "games     1.405465\n",
       "likes     0.712318\n",
       "movies    1.405465\n",
       "to        0.712318\n",
       "too       1.405465\n",
       "watch     0.712318"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index = vocab, columns=[\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "improving-search",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:44:44.620460Z",
     "start_time": "2021-04-15T04:44:44.601105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>James</th>\n",
       "      <th>John</th>\n",
       "      <th>Mary</th>\n",
       "      <th>TV</th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>likes</th>\n",
       "      <th>movies</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.424636</td>\n",
       "      <td>2.81093</td>\n",
       "      <td>1.424636</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712318</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.712318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.712318</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.712318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      James      John  Mary        TV      also       and  football     games  \\\n",
       "0  0.000000  1.405465   1.0  0.000000  0.000000  1.405465  0.000000  0.000000   \n",
       "1  1.405465  0.000000   0.0  1.405465  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000   1.0  0.000000  1.405465  0.000000  1.405465  1.405465   \n",
       "\n",
       "      likes   movies        to       too     watch  \n",
       "0  1.424636  2.81093  1.424636  1.405465  0.712318  \n",
       "1  0.712318  0.00000  0.712318  0.000000  0.712318  \n",
       "2  0.712318  0.00000  0.712318  0.000000  0.712318  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        \n",
    "        result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "built-christmas",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:47:05.602312Z",
     "start_time": "2021-04-15T04:47:05.587895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>james</th>\n",
       "      <th>john</th>\n",
       "      <th>likes</th>\n",
       "      <th>mary</th>\n",
       "      <th>movies</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>tv</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321556</td>\n",
       "      <td>0.379832</td>\n",
       "      <td>0.244551</td>\n",
       "      <td>0.643111</td>\n",
       "      <td>0.189916</td>\n",
       "      <td>0.321556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572929</td>\n",
       "      <td>0.338381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274634</td>\n",
       "      <td>0.353642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       also       and  football     games     james      john     likes  \\\n",
       "0  0.000000  0.321556  0.000000  0.000000  0.000000  0.321556  0.379832   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.572929  0.000000  0.338381   \n",
       "2  0.464997  0.000000  0.464997  0.464997  0.000000  0.000000  0.274634   \n",
       "\n",
       "       mary    movies        to       too        tv     watch  \n",
       "0  0.244551  0.643111  0.189916  0.321556  0.000000  0.189916  \n",
       "1  0.000000  0.000000  0.338381  0.000000  0.572929  0.338381  \n",
       "2  0.353642  0.000000  0.274634  0.000000  0.000000  0.274634  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn TFidVectorizer 활용\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "  'John likes to watch movies and Mary likes movies too',\n",
    "  'James likes to watch TV',\n",
    "  'Mary also likes to watch football games',  \n",
    "]\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "vocab = list(set(tfidfv.vocabulary_.keys())) # 단어장을 리스트로 저장\n",
    "vocab.sort() # 단어장을 알파벳 순으로 정렬\n",
    "\n",
    "# TF-IDF 행렬에 단어장을 데이터프레임의 열로 지정하여 데이터프레임 생성\n",
    "tfidf_ = pd.DataFrame(tfidfv.transform(corpus).toarray(), columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-notification",
   "metadata": {},
   "source": [
    "# 3-8. LSA와 LDA (1) LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "knowing-visiting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:49:46.446046Z",
     "start_time": "2021-04-15T04:49:46.444365Z"
    }
   },
   "outputs": [],
   "source": [
    "# LSA(Latent Semantic Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "civil-concrete",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T04:49:47.869344Z",
     "start_time": "2021-04-15T04:49:47.867257Z"
    }
   },
   "outputs": [],
   "source": [
    "# 특잇값 분해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-tomorrow",
   "metadata": {},
   "source": [
    "# 3-9. LSA와 LDA (2) LSA 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "african-alert",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:58:32.467060Z",
     "start_time": "2021-04-15T05:58:32.265758Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sapphire-honor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:58:41.459661Z",
     "start_time": "2021-04-15T05:58:38.035140Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/thuban/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/thuban/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/thuban/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "annoying-retailer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:59:03.707759Z",
     "start_time": "2021-04-15T05:58:56.650098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/thuban/aiffel/topic_modelling/data/abcnews-date-text.csv',\n",
       " <http.client.HTTPMessage at 0x7f59e362dd90>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드 및 확인\n",
    "import os\n",
    "\n",
    "csv_filename = os.getenv('HOME')+'/aiffel/topic_modelling/data/abcnews-date-text.csv'\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", \n",
    "                           filename=csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "imposed-length",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:59:12.608038Z",
     "start_time": "2021-04-15T05:59:11.970361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082168\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(csv_filename, error_bad_lines=False)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cathedral-championship",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:59:16.415785Z",
     "start_time": "2021-04-15T05:59:16.410715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "domestic-richardson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:59:30.771494Z",
     "start_time": "2021-04-15T05:59:30.754212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[['headline_text']]\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bronze-entertainment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:59:37.208564Z",
     "start_time": "2021-04-15T05:59:36.946015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline_text    1054983\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.nunique() # 중복을 제외하고 유일한 시퀀스를 가지는 샘플의 개수를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "elementary-median",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T05:59:46.447495Z",
     "start_time": "2021-04-15T05:59:46.162364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thuban/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "text.drop_duplicates(inplace=True) # 중복 샘플 제거\n",
    "text = text.reset_index(drop=True)\n",
    "print(len(text)) # 중복 제거 후 샘플 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "heated-grammar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:01:56.201812Z",
     "start_time": "2021-04-15T06:00:05.866005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decides, community, broadcasting, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witnesses, must, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[g, calls, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0   [aba, decides, community, broadcasting, licence]\n",
       "1    [act, fire, witnesses, must, aware, defamation]\n",
       "2     [g, calls, infrastructure, protection, summit]\n",
       "3          [air, nz, staff, aust, strike, pay, rise]\n",
       "4  [air, nz, strike, affect, australian, travellers]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 정제 및 정규화\n",
    "# NLTK 토크나이저를 이용해서 토큰화\n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
    "\n",
    "# 불용어 제거\n",
    "stop_words = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "authorized-saying",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:16:58.180151Z",
     "start_time": "2021-04-15T06:16:36.399101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [aba, decide, community, broadcast, licence]\n",
      "1    [act, fire, witness, must, aware, defamation]\n",
      "2       [call, infrastructure, protection, summit]\n",
      "3            [air, staff, aust, strike, pay, rise]\n",
      "4    [air, strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 단어 정규화. 3인칭 단수 표현 -> 1인칭 변환, 과거형 동사 -> 현재형 동사 등을 수행한다.\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "\n",
    "# 길이가 1 ~ 2인 단어는 제거.\n",
    "text = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-green",
   "metadata": {},
   "source": [
    "# 역토큰화 및 DTM 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pressed-vermont",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:17:23.524445Z",
     "start_time": "2021-04-15T06:17:20.559100Z"
    }
   },
   "outputs": [],
   "source": [
    "# 역토큰화 (토큰화 작업을 역으로 수행)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(text[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "train_data = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "heavy-teddy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:17:28.250562Z",
     "start_time": "2021-04-15T06:17:28.247269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba decide community broadcast licence',\n",
       " 'act fire witness must aware defamation',\n",
       " 'call infrastructure protection summit',\n",
       " 'air staff aust strike pay rise',\n",
       " 'air strike affect australian travellers']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "anticipated-venezuela",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:18:33.864142Z",
     "start_time": "2021-04-15T06:18:27.583606Z"
    }
   },
   "outputs": [],
   "source": [
    "# 상위 5000개의 단어만 사용\n",
    "c_vectorizer = CountVectorizer(stop_words='english', max_features = 5000)\n",
    "document_term_matrix = c_vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "continent-toilet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:19:11.129543Z",
     "start_time": "2021-04-15T06:19:11.127197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬의 크기 : (1054983, 5000)\n"
     ]
    }
   ],
   "source": [
    "print('행렬의 크기 :',document_term_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "owned-nylon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:23:33.226507Z",
     "start_time": "2021-04-15T06:23:28.192790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20398070e-02, -3.70266098e-03,  1.82589286e-02, ...,\n",
       "         1.05253020e-03,  1.39582854e-03, -1.53132626e-03],\n",
       "       [ 2.90342632e-02, -1.08693282e-02,  1.82507227e-02, ...,\n",
       "         1.06769664e-02, -2.09447680e-03,  3.47949268e-03],\n",
       "       [ 5.03561575e-03, -2.02884219e-03,  9.76304452e-03, ...,\n",
       "        -3.23490171e-03, -5.35924405e-04, -3.15345579e-04],\n",
       "       ...,\n",
       "       [ 2.96890867e-02,  4.09459620e-03,  2.48473620e-02, ...,\n",
       "         1.36227521e-02,  2.33933405e-02,  3.12577570e-02],\n",
       "       [ 6.18371728e-02, -9.05170869e-03,  1.37498166e-01, ...,\n",
       "         1.18634813e-01,  1.23151842e+00, -2.43748688e-01],\n",
       "       [ 7.12411606e-02,  3.06391356e-02,  1.40345752e-03, ...,\n",
       "         2.80461140e-02,  1.34061430e-02, -4.05965654e-03]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn TruncatedSVD 활용\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_topics = 10\n",
    "lsa_model = TruncatedSVD(n_components = n_topics)\n",
    "lsa_model.fit_transform(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "external-twist",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:23:33.881418Z",
     "start_time": "2021-04-15T06:23:33.879191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(lsa_model.components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "apart-champion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:26:23.563233Z",
     "start_time": "2021-04-15T06:26:23.553288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('police', 0.74637), ('man', 0.45357), ('charge', 0.21089), ('new', 0.14084), ('court', 0.11144)]\n",
      "Topic 2: [('man', 0.69402), ('charge', 0.3007), ('court', 0.16865), ('face', 0.11345), ('murder', 0.10672)]\n",
      "Topic 3: [('new', 0.8366), ('plan', 0.23618), ('say', 0.1822), ('govt', 0.11139), ('council', 0.11008)]\n",
      "Topic 4: [('say', 0.73885), ('plan', 0.36032), ('govt', 0.16855), ('council', 0.12252), ('urge', 0.07747)]\n",
      "Topic 5: [('plan', 0.7336), ('council', 0.17595), ('govt', 0.14085), ('urge', 0.08347), ('water', 0.06654)]\n",
      "Topic 6: [('govt', 0.53539), ('court', 0.27103), ('urge', 0.25842), ('fund', 0.22612), ('face', 0.1615)]\n",
      "Topic 7: [('charge', 0.5234), ('court', 0.45961), ('face', 0.3496), ('plan', 0.1266), ('murder', 0.11124)]\n",
      "Topic 8: [('court', 0.61303), ('crash', 0.17978), ('kill', 0.15536), ('face', 0.11675), ('accuse', 0.10641)]\n",
      "Topic 9: [('win', 0.86187), ('australia', 0.1382), ('crash', 0.11678), ('cup', 0.10068), ('world', 0.08926)]\n",
      "Topic 10: [('kill', 0.53482), ('crash', 0.51518), ('charge', 0.23805), ('car', 0.20397), ('die', 0.11198)]\n"
     ]
    }
   ],
   "source": [
    "terms = c_vectorizer.get_feature_names() # 단어 집합. 5,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(lsa_model.components_, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-premiere",
   "metadata": {},
   "source": [
    "# 3-10. LSA와 LDA (3) LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://wikidocs.net/30708"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-armstrong",
   "metadata": {},
   "source": [
    "# 3-11. LSA와 LDA (4) LDA 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "partial-evans",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:55:58.657848Z",
     "start_time": "2021-04-15T06:55:52.281708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬의 크기 : (1054983, 5000)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 행렬 생성\n",
    "# 상위 5,000개의 단어만 사용\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features= 5000)\n",
    "tf_idf_matrix = tfidf_vectorizer.fit_transform(train_data)\n",
    "\n",
    "# TF-IDF 행렬의 크기를 확인해봅시다.\n",
    "print('행렬의 크기 :',tf_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dying-listing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:59:29.925539Z",
     "start_time": "2021-04-15T06:56:38.612881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0335099 , 0.0335099 , 0.0335099 , ..., 0.17024867, 0.0335099 ,\n",
       "        0.0335099 ],\n",
       "       [0.03365631, 0.03365631, 0.03365631, ..., 0.03365631, 0.03365631,\n",
       "        0.03365631],\n",
       "       [0.25184095, 0.0366096 , 0.0366096 , ..., 0.0366096 , 0.0366096 ,\n",
       "        0.0366096 ],\n",
       "       ...,\n",
       "       [0.26687206, 0.02914502, 0.02914502, ..., 0.13007484, 0.02916018,\n",
       "        0.28739608],\n",
       "       [0.10378115, 0.02637829, 0.12325014, ..., 0.02637829, 0.02637829,\n",
       "        0.02637829],\n",
       "       [0.03376055, 0.03376055, 0.2255442 , ..., 0.03376055, 0.03376055,\n",
       "        0.03376055]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn LDA Model 활용\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', random_state=777, max_iter=1)\n",
    "lda_model.fit_transform(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "level-production",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:59:30.656802Z",
     "start_time": "2021-04-15T06:59:30.654376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(lda_model.components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "angry-pricing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T06:59:31.420746Z",
     "start_time": "2021-04-15T06:59:31.410834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('australia', 9359.06334), ('sydney', 5854.97288), ('attack', 4784.76322), ('change', 4193.63035), ('year', 3924.88997)]\n",
      "Topic 2: [('government', 6344.07413), ('charge', 5947.12292), ('man', 4519.7974), ('state', 3658.16422), ('live', 3625.10473)]\n",
      "Topic 3: [('australian', 7666.65651), ('say', 7561.01807), ('police', 5513.22932), ('home', 4048.38409), ('report', 3796.04446)]\n",
      "Topic 4: [('melbourne', 5298.35047), ('south', 4844.59835), ('death', 4281.78433), ('china', 3214.44581), ('women', 3029.28443)]\n",
      "Topic 5: [('win', 5704.0914), ('canberra', 4322.0963), ('die', 4025.63057), ('open', 3771.65243), ('warn', 3577.47151)]\n",
      "Topic 6: [('court', 5246.3124), ('world', 4536.86331), ('country', 4166.34794), ('woman', 3983.97748), ('crash', 3793.50267)]\n",
      "Topic 7: [('election', 5418.5038), ('adelaide', 4864.95604), ('house', 4478.6135), ('school', 3966.82676), ('2016', 3955.11155)]\n",
      "Topic 8: [('trump', 8189.58575), ('new', 6625.2724), ('north', 3705.40987), ('rural', 3521.42659), ('donald', 3356.26657)]\n",
      "Topic 9: [('perth', 4552.8151), ('kill', 4093.61782), ('break', 2695.71958), ('budget', 2596.93268), ('children', 2586.01957)]\n",
      "Topic 10: [('queensland', 5552.68506), ('coast', 3825.32603), ('tasmanian', 3550.75997), ('shoot', 3185.71575), ('service', 2695.21462)]\n"
     ]
    }
   ],
   "source": [
    "# LDA의 결과 토픽과 각 단어의 비중을 출력합시다\n",
    "terms = tfidf_vectorizer.get_feature_names() # 단어 집합. 5,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "get_topics(lda_model.components_, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-punishment",
   "metadata": {},
   "source": [
    "# 3-12. 텍스트 분포를 이용한 비지도 학습 토크나이저 (1) 형태소 분석기와 단어 미등록 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "continent-latino",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:27:50.207540Z",
     "start_time": "2021-04-15T07:27:50.204530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'ran', 'back', 'to', 'the', 'corner', 'near', 'the', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석기의 필요성\n",
    "en_text = \"The dog ran back to the corner near the spare bedrooms\"\n",
    "print(en_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "duplicate-necklace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:28:10.613688Z",
     "start_time": "2021-04-15T07:28:10.611036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사', '왔어']\n"
     ]
    }
   ],
   "source": [
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사 왔어\"\n",
    "print(kor_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "standing-leather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:28:33.598610Z",
     "start_time": "2021-04-15T07:28:28.323015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과', '의', '놀라운', '효능', '이라는', '글', '을', '봤어', '.', '그래서', '오늘', '사과', '를', '먹으려고', '했는데', '사과', '가', '썩어서', '슈퍼', '에', '가서', '사과', '랑', '오렌지', '사', '왔어']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "tokenizer = Okt()\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "general-debut",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:28:42.066776Z",
     "start_time": "2021-04-15T07:28:42.044022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['모두', '의', '연구소', '에서', '자연어', '처리', '를', '공부', '하는', '건', '정말', '즐거워']\n"
     ]
    }
   ],
   "source": [
    "# 단어 미등록 문제\n",
    "from konlpy.tag import Okt\n",
    "tokenizer = Okt()\n",
    "print(tokenizer.morphs('모두의연구소에서 자연어 처리를 공부하는 건 정말 즐거워'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-noise",
   "metadata": {},
   "source": [
    "# 3-13. 텍스트 분포를 이용한 비지도 학습 토크나이저 (2) soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-zealand",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
