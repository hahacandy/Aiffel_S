{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worth-corner",
   "metadata": {},
   "source": [
    "# 3-1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-spectacular",
   "metadata": {},
   "source": [
    "$ mkdir -p ~/aiffel/topic_modelling/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-cricket",
   "metadata": {},
   "source": [
    "# 3-2. 단어 빈도를 이용한 벡터화 (1) Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turkish-circus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:25:41.145008Z",
     "start_time": "2021-04-15T02:25:41.142281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-malta",
   "metadata": {},
   "source": [
    "# 3-3. 단어 빈도를 이용한 벡터화 (2) Bag of Words 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "young-heart",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:26:47.331484Z",
     "start_time": "2021-04-15T02:26:46.168962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words : {'john': 1, 'likes': 3, 'to': 2, 'watch': 2, 'movies': 2, 'mary': 2, 'too': 1, 'also': 1, 'football': 1, 'games': 1}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "# keras Tokenizer 활용\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
    "\n",
    "def print_bow(sentence):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence) # 단어장 생성\n",
    "    bow = dict(tokenizer.word_counts) # 각 단어와 각 단어의 빈도를 bow에 저장\n",
    "    print(\"Bag of Words :\", bow) # bow 출력\n",
    "    print('단어장(Vocabulary)의 크기 :', len(tokenizer.word_counts)) # 중복을 제거한 단어들의 개수\n",
    "\n",
    "print_bow(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expired-digest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:29:29.773082Z",
     "start_time": "2021-04-15T02:29:29.769170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[1 1 1 1 3 2 2 2 1 2]]\n",
      "각 단어의 인덱스 : {'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn CountVectorizer 활용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
    "vector = CountVectorizer()\n",
    "print('Bag of Words : ',vector.fit_transform(sentence).toarray()) # 코퍼스로부터 각 단어의 빈도수를 기록한다.\n",
    "print('각 단어의 인덱스 :',vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
    "print('단어장(Vocabulary)의 크기 :', len(vector.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-rebate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:28:16.651872Z",
     "start_time": "2021-04-15T02:28:16.649234Z"
    }
   },
   "source": [
    "# 3-4. 단어 빈도를 이용한 벡터화 (3) DTM과 코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recovered-scholarship",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:38:50.981036Z",
     "start_time": "2021-04-15T02:38:50.977780Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "doc1 = np.array([0,1,1,1]) # 문서1 벡터\n",
    "doc2 = np.array([1,0,1,1]) # 문서2 벡터\n",
    "doc3 = np.array([2,0,2,2]) # 문서3 벡터\n",
    "\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clinical-insulin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:40:03.956612Z",
     "start_time": "2021-04-15T02:40:03.953493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "0.6666666666666667\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(doc1, doc2)) #문서1과 문서2의 코사인 유사도\n",
    "print(cos_sim(doc1, doc3)) #문서1과 문서3의 코사인 유사도\n",
    "print(cos_sim(doc2, doc3)) #문서2과 문서3의 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-corrections",
   "metadata": {},
   "source": [
    "# 3-5. 단어 빈도를 이용한 벡터화 (4) DTM의 구현과 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "impressed-player",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:40:55.648841Z",
     "start_time": "2021-04-15T02:40:55.634217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 1 0 1]\n",
      " [0 0 0 0 1 1 1 0 1 0]\n",
      " [1 1 1 0 1 1 0 1 0 1]]\n",
      "{'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn CountVectorizer 활용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'John likes to watch movies',\n",
    "    'Mary likes movies too',\n",
    "    'Mary also likes to watch football games',    \n",
    "]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도수를 기록.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-volleyball",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T02:43:05.946097Z",
     "start_time": "2021-04-15T02:43:05.944266Z"
    }
   },
   "source": [
    "# 3-6. 단어 빈도를 이용한 벡터화 (5) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-rubber",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
